{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization for Simplicity: L2 (ridge) Regularization \n",
    "\n",
    "This exercise is based on the tensorflow [playground](https://playground.tensorflow.org) program (developed by google to teach machine learning principles).\n",
    "You'll experiment L2 regularization for a small, noisy training data set to perform 'supervised) binary classification. In this kind of setting, overfitting is a real concern. Fortunately, regularization might help.\n",
    "\n",
    "The input data are bivariate, this yields the two features $X_1$ and $X_2$. Feature crosses such that their product $X_1X_2$, their squared values $X_1^2$ and $X_2^2$, or their sinus are also included as input for the linear model.\n",
    "\n",
    "This exercise consists of three related tasks. To simplify comparisons across the three tasks, run each task in a separate tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "Run this [tensorflow playground model](https://playground.tensorflow.org/#activation=linear&regularization=L2&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.01&regularizationRate=0&noise=50&networkShape=&seed=0.32738&showTestData=false&discretize=false&percTrainData=20&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false&playButton_hide=false&regularizationRate_hide=false&percTrainData_hide=false&numHiddenLayers_hide=true&noise_hide=false&problem_hide=true&regularization_hide=false&dataset_hide=false&activation_hide=true) as given for at least 500 epochs. Note the following:\n",
    "  - Test loss.\n",
    "  - The delta between Test loss and Training loss.\n",
    "  - The learned coefficients of the features and the feature crosses. (The relative thickness of each line running from INPUT (features) to OUTPUT (response) variables represents the learned coefficient for that feature, or feature cross. You can find the exact coefficient values by hovering over each line.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: \n",
    "(Consider doing this Task in a separate tab.) \n",
    "\n",
    "Consider the same model, and increase the regularization rate from $0$ to $0.3$. Then, run the model for at least 500 epochs and find answers to the following questions:\n",
    " - How does the Test loss in Task 2 differ from the Test loss in Task 1?\n",
    " - How does the delta between Test loss and Training loss in Task 2 differ from that of Task 1?\n",
    " - How do the learned ceofficients of each feature and feature cross differ from Task 2 to Task 1?\n",
    " - What do your results say about model complexity?\n",
    " - what happens when the regularization rate is too high?\n",
    " - which value for the regularization seems optimal?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* Given the randomness in the data set, it is impossible to assert which regularization rate produced the best results for you. But in many cases, we will retrieve the same behavior and similar values for the _interesting_ learning rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
