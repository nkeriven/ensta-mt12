{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/04a_linear_model_ridge/N3_ridge_NIR_biscuits.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adapted from the following [R notebook](https://github.com/wikistat/Apprentissage/blob/master/NIR/Apprent-R-cookies.ipynb) from [wikistat.fr](http://wikistat.fr/)*\n",
    "\n",
    "# NIR Biscuits dataset\n",
    "\n",
    "Prediction in large dimension $d > n$ where the input variables are the discretizations of near infrared (NIR) spectra and the target (responses) the sugar content in a cake dough. \n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "This data set contains measurements from quantitative NIR spectroscopy. The example studied arises from an experiment done to test the feasibility of NIR spectroscopy to measure the composition of biscuit dough pieces (formed but unbaked biscuits). Two similar sample sets were made up, with the standard recipe varied to provide a large range for each of the four constituents under investigation: fat, sucrose, dry flour, and water. The calculated percentages of these four ingredients represent the 4 responses. There are 40 samples in the training set (with sample 23 being an outlier) and a further 32 samples in the test set (with example 21 considered as an outlier).\n",
    "- An NIR reflectance spectrum is available for each dough piece. The spectral data consist of 700 points measured from 1100 to 2498 nanometers (nm) in steps of 2 nm. \n",
    "- The last four variables are the responses $y$ which are in order the percentages of _fat_, _sucrose_, _dry flour_, and _water_.\n",
    "\n",
    "The original data are due to _Osbone et al. (1984)_ and have often been used for method comparison (see e.g. _Krämer et al. 2008_). \n",
    "> - B. G. Osborne, T. Fearn, A. R. Miller et S. Douglas (1984). Application of Near Infrared Reflectance spectroscopy to the compositional analysis of biscuits and biscuit doughs, J. Sci. Food Agric. 35, 99–105.\n",
    "> - Nicole Krämer, Anne Laure Boulesteix et Gerhard Tutz (2008). Penalized Partial Least Squares with applications to B-spline transformations and functional data, Chemometrics and Intelligent Laboratory Systems 94, 60–69.\n",
    "\n",
    "## Task\n",
    "\n",
    "An accurate prediction of the cookie composition from the NIR spectra is a lot of interest in industrial applications to save time, and therefore money. The study is restricted to the modelling of sugar levels only, in order to find a better forecasting model. Different regression methods are used and their predictive qualities are compared.\n",
    "\n",
    "\n",
    "## Exercise\n",
    "\n",
    "In the following, the questions to answer (exercises) are indicated by a **Q** mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/cookie.csv')\n",
    "print(df.shape)\n",
    "#Show first lines\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format the input/target data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training test is composed of the first 40 samples:\n",
    "# input are the NIR spectra along the 700 wavelength (1100nm to 2498nm)\n",
    "Xtrain = df.iloc[:40, :700].to_numpy()\n",
    "# target is the 'sucrose' variable\n",
    "Ytrain = df.iloc[:40, 701].to_numpy()\n",
    "# Test test is composed of the last 32 samples:\n",
    "Xtest = df.iloc[40:, :700].to_numpy()\n",
    "Ytest = df.iloc[40:, 701].to_numpy()\n",
    "print('Xtrain shape: {}, Ytrain shape: {}'.format(Xtrain.shape, Ytrain.shape))\n",
    "print('Xtest  shape: {}, Ytest  shape: {}'.format(Xtest.shape, Ytest.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the training set\n",
    "For each training sample $i=1\\ldots 40$, plot the spectrum $X_i \\in \\mathbb{R}^{700}$ with a specific color depending on the targetted sugar content $y_i$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "def multiline(data, y_color, x_axis=None,  ax=None, **kwargs):\n",
    "    # find axes\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "    # create LineCollection\n",
    "    x_axis = np.arange(data.shape[1]) if x_axis is None else x_axis\n",
    "    segments = [np.column_stack([x_axis, x]) for x in data]\n",
    "    lc = LineCollection(segments, **kwargs)\n",
    "    # set coloring of line segments\n",
    "    lc.set_array( np.asarray(y_color) )\n",
    "    # add lines to axes and rescale \n",
    "    ax.add_collection(lc)\n",
    "    ax.autoscale()\n",
    "    return lc\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "wavelengths = np.linspace(1100,2498,700) # nanometers\n",
    "lc = multiline(Xtrain,  Ytrain,  x_axis=wavelengths, ax=ax)\n",
    "ax.set_title('X data (NIR Spectra)')\n",
    "axcb = fig.colorbar(lc)\n",
    "axcb.set_label('Y (% of sugar)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** : What do you think about the smoothness of these spectra and the possible correlation between the input variables for the considered wavelength discretization\n",
    "\n",
    "**Q** : By visual inspection, is there a relationship between NIR spectra (input variables) and sugar content (target)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display residuals (responses used as global variables)\n",
    "def plot_residuals(ax, msg):\n",
    "    ax.plot(Ytrain-Ytrain_hat,'s',label='train')\n",
    "    ax.plot(Ytest-Ytest_hat,'o',label='test')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('sample index')\n",
    "    ax.set_title(msg+ ' residuals')\n",
    "    ax.grid('On')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# fit a linear model with ordinary least squares estimate\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(Xtrain, Ytrain)\n",
    "# make Prediction \n",
    "Ytrain_hat =  linreg.predict(Xtrain)\n",
    "Ytest_hat =  linreg.predict(Xtest)\n",
    "# plot the coeff\n",
    "_, axes = plt.subplots(1,2,figsize=(15,5))\n",
    "axes[0].plot(wavelengths,linreg.coef_)\n",
    "axes[0].set_xlabel('wavelengths')\n",
    "axes[0].set_title('Estimated coefficients')\n",
    "# plot the residuals\n",
    "plot_residuals(axes[1], 'linear regression')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Q**: How can we explain that the residuals are all zero on the training set?\n",
    "- **Q**: What are the properties of the returned regressor? Hint: see https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html\n",
    "- **Q**: Do you think that linear regression fit with ordinary least squares estimation is suitable here (explain why)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the prediction *score*\n",
    "\n",
    "The usual squared $\\ell_2$ loss, which yields the _mean squared error_, depends on the scale of the response $y$ which is dataset dependent and/or can be arbitrarily scaled. Thus to appreciate the performance of the prediction, it may be more meaningful to consider a normalized measure such that the __coefficient of determination__, usually denoted as $R^2$.\n",
    "\n",
    "This represents the proportion of variance (of y) that has been explained by the model prediction:\n",
    "Best possible score (the higher, the better)  is $1$ and it can be negative (because the model can be arbitrarily worse). A (trivial) constant model that always predicts the expected value of y, disregarding the input features, would get a $R^2$ score of 0.\n",
    "\n",
    "\n",
    "If $\\hat{y}_i$ is the predicted value of the $i$-th sample and $y_i$ is the corresponding true value for $i=1,\\ldots,n$, the estimated (biased) $R^2$ is defined as:\n",
    "$$\n",
    "R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "where\n",
    "$\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$\n",
    "is the sample mean of the responses,\n",
    "$\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} \\epsilon_i^2$\n",
    "is the residual sum of squares (mean squared error estimate), and \n",
    "$\\sum_{i=1}^{n} (y_i - \\bar{y})^2$ \n",
    "is the (unnormalized) sample variance of the responses, called the _total variance_.\n",
    "\n",
    "When it is estimated based on **Test data**, this provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print('Linear regression : R^2 is {:6.4f}'.format(r2_score(Ytest,Ytest_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What strategies can be investigated to improve the performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component regression (PCR)\n",
    "\n",
    "In order to reduce the dimension of the problem and to prevent the curse of dimensionality, one simple strategy is to used as regressors the principal components (PCA procedure) of the input variables. Principal component regression (PCR) typically uses only a subset of all the principal components for regression, making PCR a kind of _regularized procedure_.\n",
    "With scikit-learn pipelines, this is super easy to link the two procedures to make PCR.\n",
    "\n",
    "**Q**: Why PCR procedure may allows one to mitigate overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the PCR prediction procedure as a pipeline of pca + linear regression steps\n",
    "# Also remember that scale matters for PCA (and more generally for regularized procedure)!\n",
    "pcr = Pipeline( [('scaler', StandardScaler()), ('pca', PCA()), ('linreg', LinearRegression())] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What are the hyperparameters to be optimized for PCR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cross-validation procedure\n",
    "pcrCV = GridSearchCV(pcr, {'pca__n_components': list(range(1, 10)) + [10,20] }, n_jobs=1, cv=10)\n",
    "pcrCV.fit(Xtrain,Ytrain)\n",
    "# make Predistion \n",
    "Ytrain_hat =  pcrCV.predict(Xtrain)\n",
    "Ytest_hat =  pcrCV.predict(Xtest)\n",
    "# plot the residuals\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plot_residuals(ax, 'PCR')\n",
    "# display best cross-validated hyperparameters\n",
    "print('CV estimated PCR hyperparameters: {}'.format(pcrCV.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute R2 score\n",
    "print('Principal component regression: R^2 is {:6.4f}'.format(r2_score(Ytest,Ytest_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Q**: what is the dimension reduction ratio with respect to (full) linear regression?\n",
    "- **Q**: why the number of principal components to be considered can not exceed here $36$ for the $K=10$ fold cross-validated PCR?\n",
    "- **Q**: Does it worth to use here PCR rather than linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "In order to regularize the linear regression procedure, we  consider now a ridge penalization. With scikit-learn, we can directly use `RidgeCV` to perform\n",
    "Ridge regression with built-in cross-validation.\n",
    "\n",
    "**Q**: Why ridge regression is referred to as a *shrinkage* procudure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set possible values for the regularization coefficient \n",
    "# (denoted as \\lambda in the slides or alpha in the sklearn API)\n",
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "ridge = RidgeCV(alphas=alphas, cv=10)\n",
    "# make predistion \n",
    "ridge.fit(Xtrain,Ytrain)\n",
    "Ytrain_hat =  ridge.predict(Xtrain)\n",
    "Ytest_hat = ridge.predict(Xtest)\n",
    "# plot the residuals\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plot_residuals(ax, 'Ridge regression')\n",
    "# display best cross-validated hyperparameters \n",
    "print('CV estimated ridge regularization coeff alpha: {:6.4f}'.format(ridge.alpha_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute R2 score\n",
    "print('Ridge regression: R^2 is {:6.4f}'.format(r2_score(Ytest,Ytest_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Q**: Based on all these results, is there valuable information in NIR spectra to predict sucrose content? What is the standard error magnitude for predicting the percentage of sucrose?\n",
    "- **Q**: *Optional:* Redo the same analysis now for the `dry flour` response variable . Do you find the same results and conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation curves\n",
    "Display cross-validation curves to select the optimal regularization parameter. Note that this requires now to use `Ridge`+ `GridSearchCV` sklearn methods (rather than the direct`RidgeCV`) to have access to the $K$ fold cross valiation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "ridge = Ridge()\n",
    "cv = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "model = GridSearchCV(ridge, {'alpha': np.logspace(-4, 1, 30) }, cv=cv, scoring='r2')\n",
    "model.fit(Xtrain,Ytrain)\n",
    "\n",
    "# Display results\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "alphas = np.asarray( model.cv_results_['param_alpha'], dtype=float)\n",
    "scores = model.cv_results_['mean_test_score']\n",
    "std_error = model.cv_results_['std_test_score']\n",
    "plt.semilogx(alphas, scores , 'g-',label='mean')\n",
    "ax.grid()\n",
    "ax.semilogx(alphas, scores + std_error, 'g--')\n",
    "ax.semilogx(alphas, scores - std_error, 'g--', label='+/-std')\n",
    "ax.fill_between(alphas, scores - std_error, scores + std_error, \n",
    "                alpha=0.2, color=\"g\")\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "ax.set_ylabel(r'CV $R^2$ score +/- std error')\n",
    "ax.set_title(r'Coefficient of determination $R^2$ estimated on {} folds'.format(cv.n_splits))\n",
    "ax.axhline(np.max(scores), linestyle=':', color='.5', label='best')\n",
    "ax.legend()\n",
    "ax.axis('tight')\n",
    "ax.set_xlim([alphas[0], alphas[-1]])\n",
    "Ytest_hat = model.predict(Xtest)\n",
    "print('Ridge regression: R^2 is {:6.4f}'.format(r2_score(Ytest,Ytest_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Q**: How confident are you in the estimation of the $R^2$ score by cross-validation?\n",
    "- **Q**: How confident are you in the estimation of the best regularization coefficient alpha by cross-validation (*hint:* randomize the CV folds by setting `shuffle=True` and changing the `random_state`) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (*Optional*) Leave-one-out Cross Validation (LOOCV)\n",
    "\n",
    "Remember that Leave-one-out cross-validation (LOOCV) takes only *one left-out sample* as the validation set and the remaining observations as the training set. This is repeated until each sample is left-out.\n",
    "\n",
    "\n",
    "Using some algebra tricks, LOOCV can be computed very efficiently (in fact, essentially *for free*) for ridge regression models as explained in\n",
    "> Rifkin M. Ryan, and Ross A. Lippert. [\"Notes on regularized least squares,\" (2007)](http://128.30.100.62:8080/media/fb/ps/MIT-CSAIL-TR-2007-025.pdf), MIT-CSAIL Technical Report, \n",
    "\n",
    "Note also that there exist a famous variant of LOOCV based on a rotation of the original regression problem (called generalized cross validation): \n",
    "> Golub G., Heath M., and Wahba G., [\"Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter,\" (1979)](http://pages.stat.wisc.edu/~wahba/stat860public/pdf1/golub.heath.wahba.pdf), TECHNOMETRICS, Vol 21, No 2\n",
    "\n",
    "which appears to be quite useful on many real-word datasets.\n",
    "\n",
    "*LOO is the default internal procedure for sklearn `RidgeCV` method (when `cv`parameter is None)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LOOCV (default setting)\n",
    "alphas = np.logspace(-5, 0.5, 100)\n",
    "ridge = RidgeCV(alphas=alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Q**: What are the best regularization coefficient $\\alpha$ and the estimated $R^2$  for LOOCV on this dataset?\n",
    "- **Q**: Comparing with the performance obtained on the test set (and with the previous $K$-fold CV), is LOO worthy here? How can we explain this result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
