{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/07_MLP_NN/N2_MLPClassifier.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptron Classifier\n",
    "\n",
    "see https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "\n",
    "This first example aims at illustrating the interest of multilayer perceptron in a decision problem which is not linearly separable.  \n",
    "The principle is identical to the principle of the simple (single layer) perceptron, but extends the learner strucrure to many layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example of boolean functions\n",
    "###  Create and visualize data for the logical \"AND\" and \"OR\"\n",
    "$$ y = x_1 \\& x_2 $$ $$ y = x_1 \\text{ or } x_2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logical AND\n",
    "X=np.array([[0.,0.],[1.,1.],[1,0],[0,1]])\n",
    "y=np.array([[0,1,0,0],[0,0,1,1]]) # AND on row 1, OR on row 2\n",
    "\n",
    "for i in range(2):\n",
    "    plt.figure()\n",
    "    plt.scatter(X[y[i,:]==0,0],X[y[i,:]==0,1], c='r')\n",
    "    plt.scatter(X[y[i,:]==1,0],X[y[i,:]==1,1], c='b')\n",
    "    plt.title('AND' if i==0 else 'OR')\n",
    "            \n",
    "print('red = 0 ; blue=1  : ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "In the 2 examples above,\n",
    "- are the 2 clusters linearly separable? \n",
    "- how many layers are necessary to separate the two clusters?\n",
    "- Would a Perceptron (similar to the the one studied in N1_Perceptron.ipynb) be an acceptable solution? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "We are going to implement a Perceptron as an MLP with *zero* hidden layer.\n",
    "\n",
    "see https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "\n",
    "for more information about the function and input/output prarameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is just a perceptron !!\n",
    "clf_lin = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(), random_state=1, activation='logistic')\n",
    "\n",
    "# prepare to plot\n",
    "xx=np.linspace(-.1,1.1,50)\n",
    "yy=np.linspace(-.1,1.1,50)\n",
    "XX,YY= np.meshgrid(xx,yy)\n",
    "XY = np.vstack([XX.flatten(), YY.flatten()]).T\n",
    "\n",
    "for i in range(2):\n",
    "    # fit the model\n",
    "    clf_lin.fit(X,y[i,:])\n",
    "    Z=clf_lin.predict(XY)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(X[y[i,:]==0,0],X[y[i,:]==0,1], c='r')\n",
    "    plt.scatter(X[y[i,:]==1,0],X[y[i,:]==1,1], c='b')\n",
    "    plt.scatter(XY[Z==0,0],XY[Z==0,1],s=1,c='r',marker='.')\n",
    "    plt.scatter(XY[Z==1,0],XY[Z==1,1],s=1,c='b',marker='.')\n",
    "\n",
    "    print('coefficient matrix sizes : ', [coef.shape for coef in clf_lin.coefs_])\n",
    "    print(f'MLPC coefs_ :{clf_lin.coefs_}')\n",
    "    print(f'MLPC intercepts_ :{clf_lin.intercepts_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear MLP\n",
    "\n",
    "Now we add a hidden layer, to get a non-linear decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hidden layer of size 3\n",
    "clf_nonlin = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3,), random_state=1, activation='logistic')\n",
    "\n",
    "for i in range(2):\n",
    "    # fit the model\n",
    "    clf_nonlin.fit(X,y[i,:])\n",
    "    Z=clf_nonlin.predict(XY)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(X[y[i,:]==0,0],X[y[i,:]==0,1], c='r')\n",
    "    plt.scatter(X[y[i,:]==1,0],X[y[i,:]==1,1], c='b')\n",
    "    plt.scatter(XY[Z==0,0],XY[Z==0,1],s=1,c='r',marker='.')\n",
    "    plt.scatter(XY[Z==1,0],XY[Z==1,1],s=1,c='b',marker='.')\n",
    "    \n",
    "    print('coefficient matrix sizes : ', [coef.shape for coef in clf_nonlin.coefs_])\n",
    "    print(f'MLPC coefs_ :{clf_nonlin.coefs_}')\n",
    "    print(f'MLPC intercepts_ :{clf_nonlin.intercepts_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "- Draw the learner structure and its edges with corresponding weights. (hint: play with hidden_layer_sizes)\n",
    "- What is the activation function used in this MLP? \n",
    "- In the MLP computation above, 'lbfgs' is used as a solver, not 'sgd'. Explain why?\n",
    "- take hidden_layer_sizes = (1,). Explain what you see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. More realistic classification toy problem (concentric clusters)\n",
    "This example is the same as in section 7. of N1_Perceptron.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std=.4\n",
    "N=500\n",
    "\n",
    "circ_coord = np.zeros((N, 2))\n",
    "circ_coord[:,0] = 2*np.pi*np.random.rand(N)\n",
    "circ_coord[:int(N/2), 1] = np.abs(.1 + .5*circ_coord[:int(N/2), 0] + std*np.random.randn(int(N/2)))\n",
    "circ_coord[int(N/2):, 1] = np.abs(1 + .8*circ_coord[int(N/2):, 0] + std*np.random.randn(int(N/2)))\n",
    "\n",
    "X = np.zeros((N,2))\n",
    "X[:,0] = circ_coord[:,1]*np.sin(circ_coord[:,0])\n",
    "X[:,1] = circ_coord[:,1]*np.cos(circ_coord[:,0])\n",
    "\n",
    "label = np.zeros(N)\n",
    "label[:int(N/2)]=1\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1],c=label,s=10)\n",
    "plt.axis('square')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn a MLP Classifier : \n",
    "##########################\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,3), \\\n",
    "                    random_state=1, activation='relu', \\\n",
    "                    max_iter=2500)\n",
    "clf.fit(X,label)\n",
    "\n",
    "\n",
    "# Visualize some results\n",
    "########################\n",
    "\n",
    "plt.figure(figsize=[15,4])\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(X[:,0],X[:,1],c=label,s=10)\n",
    "plt.axis('square');\n",
    "plt.title('original data')\n",
    "\n",
    "# sample the observation space :\n",
    "xx=np.linspace(-7,7,100)\n",
    "yy=np.linspace(-7,7,100)\n",
    "XX,YY= np.meshgrid(xx,yy)\n",
    "XY = np.vstack([XX.flatten(), YY.flatten()]).T\n",
    "# Apply MLP Classif to the samples : \n",
    "Z=clf.predict(XY)\n",
    "Pr=clf.predict_proba(XY)\n",
    "Pr0=Pr[:,0]\n",
    "Pr1=Pr[:,1]\n",
    "\n",
    "# plot the probability values over the observation space :\n",
    "plt.subplot(132)\n",
    "Pr0=np.reshape(Pr0,[xx.size,yy.size])\n",
    "plt.contourf(xx,yy,Pr0, levels=20)\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:,0],X[:,1],c=label,s=10)\n",
    "plt.axis('square')\n",
    "plt.title('proba of being in outside spiral')\n",
    "plt.xlabel('x_1')\n",
    "plt.xlabel('x_2')\n",
    "plt.subplot(133)\n",
    "Pr1=np.reshape(Pr1,[xx.size,yy.size])\n",
    "plt.contourf(xx,yy,Pr1, levels=20)\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:,0],X[:,1],c=label,s=10)\n",
    "plt.axis('square')\n",
    "plt.title('proba of being in inside spiral')\n",
    "plt.xlabel('x_1')\n",
    "plt.xlabel('x_2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- Run the learning process many times with input paramaters by changing hidden_layer_sizes. Comment your findings. Propose an interpretation.\n",
    "- Increase the number of neurons in the first and second layer and comment your observation. Can you explain your findings? \n",
    "- What is different between this method and the method used in N1_Perceptron.ipynb notebook?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
