{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/07_MLP_NN/N1_Perceptron.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERCEPTRON introductory examples \n",
    "\n",
    "\n",
    "This code illustrates the behaviour of the perceptron using as many inputs as coordinates in \n",
    "an input sample (row), on a binary classification problem. The output decision function is the \n",
    "Heaviside step function. \n",
    "\n",
    "## 1. Description of the  toy data set\n",
    "The data set below consists in a set of 10 samples in $\\mathbb{R}^2$, containing 2 classes, as shown on the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.array([[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]])\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "#data=np.array(dataset)\n",
    "#print(data.shape)\n",
    "\n",
    "dataN=X[y==0,:]\n",
    "dataP=X[y==1,:]\n",
    "plt.plot(dataP[:,0],dataP[:,1],'or');\n",
    "plt.plot(dataN[:,0],dataN[:,1],'ob');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a single neuron, with a threshold  step activation function \n",
    "\n",
    "**\" Predict\"** function defined below,  computes \n",
    "$$ {\\rm activation} = \\sum_{k=0}^{{\\rm dim}(X)} w[k].X[k] $$\n",
    "where  $w[0]={\\rm bias}$, and $w[k]$, $k=1,\\ldots{\\rm dim}(X)$ are the neuron inputs.  \n",
    "The classification rule is defined at the output of the threshold function as \n",
    "$$ \\hat{y} = \\left\\{ \\begin{array}{ll} +1  \\mbox{ if activation } \\geq 0 \\\\\n",
    "-1 \\mbox{ if activation } < 0 \\end{array} \\right. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(sample, weights):\n",
    "    activation = weights[0] + (weights[1:]).dot(sample)\n",
    "    #for i in range(len(sample)):\n",
    "    #    activation += weights[i + 1] * sample[i]\n",
    "    #print(activation)\n",
    "    prediction = 1 if activation >= 0.0 else 0\n",
    "    return prediction,activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "- Show that the predict function defines an separation hyperplan in $\\mathbb{R}^{{\\rm dim}(X)}$\n",
    "- Express the equation of the separation line as a function of the $\\{w[k]\\}$ in the case dim$(X)$=2\n",
    "- Show in that latter case that setting $w[2]$=0 amounts to define a threshold on the first coordinate\n",
    "- Propose a set of values for $\\{w[0],w[1],w[2]\\}$ which defines a good classifier for the data above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply 'Predict' to the data samples and vizualize the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=np.array([-4.5, 1, 0])\n",
    "\n",
    "for (sample, label) in zip(X,y):\n",
    "    prediction,activation = predict(sample, weights)\n",
    "    print(f\"Expected={label}, Predicted={prediction}, activation={activation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the result for this classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=np.linspace(0,10,50)\n",
    "yy=np.linspace(-1,4.5,50)\n",
    "XX,YY= np.meshgrid(xx,yy)\n",
    "XY = np.vstack([XX.flatten(), YY.flatten()]).T\n",
    "Z=[]\n",
    "\n",
    "weights=np.array([-4.5, 1, 0])\n",
    "for sample in XY:\n",
    "    z,a=predict(sample,weights)\n",
    "    Z.append(z)\n",
    "    \n",
    "Z=np.asarray(Z)\n",
    "\n",
    "plt.plot(dataP[:,0],dataP[:,1],'or')\n",
    "plt.plot(dataN[:,0],dataN[:,1],'ob')\n",
    "plt.scatter(XY[Z==0,0],XY[Z==0,1],s=1,c='b',marker='.')\n",
    "plt.scatter(XY[Z==1,0],XY[Z==1,1],s=1,c='r',marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "* What is the value of the bias (or intercept)? \n",
    "* Is this solution unique? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learning weights, using SGD (Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD means that the optimization is conducted by taking one sample at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(X, y, l_rate, n_epoch):\n",
    "    weights = np.zeros(X.shape[1]+1)#[0.0 for i in range(len(train[0]))]  # init weights are set to 0. \n",
    "    for epoch in range(n_epoch): # here, the number of epochs is set a priori. \n",
    "        sum_error = 0.0\n",
    "        for n in range(len(y)):  # .... for each observation,\n",
    "            sample, label = X[n,:], y[n]\n",
    "            #sample=row[:-1]\n",
    "            prediction, activation = predict(sample, weights)\n",
    "            error = label - prediction  # row[-1] refers to the last element in row (here, it is the label)\n",
    "            sum_error += error**2\n",
    "            weights[0] = weights[0] + l_rate * error  # l_rate = learing rate. \n",
    "            weights[1:] = weights[1:] + l_rate * error * sample\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    return weights\n",
    "\n",
    "l_rate = .1\n",
    "n_epoch = 4\n",
    "weights = train_weights(X, y, l_rate, n_epoch)\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "- How many operation (take only multiplication into account) are necessary to complete an epoch in the function train_weights defined above? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Apply the weights estimated by SGD and visualize classication results\n",
    "\n",
    "This section proposes 3 methods to visualize the classification results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of labels using new weights\n",
    "\n",
    "# Visualization\n",
    "\n",
    "plt.figure(figsize=[12,4])\n",
    "\n",
    "# plot the separating hyperplane equation\n",
    "plt.subplot(131)   \n",
    "xx=np.linspace(0,8.7,50)\n",
    "xy = (-weights[0]-weights[1]*xx)/weights[2] # just w0 + w.x = 0\n",
    "\n",
    "plt.plot(dataP[:,0],dataP[:,1],'or')\n",
    "plt.plot(dataN[:,0],dataN[:,1],'ob')    \n",
    "plt.axis([0, 9, -1, 4.5])\n",
    "plt.plot(xx,xy);\n",
    "\n",
    "# test the sampled observation plane and plot the decision region \n",
    "plt.subplot(132) \n",
    "yy=np.linspace(-1,8,50)\n",
    "XX,YX= np.meshgrid(xx,yy)\n",
    "XY = np.vstack([XX.flatten(), YY.flatten()]).T\n",
    "Z=[]\n",
    "A=[]\n",
    "for sample in XY:\n",
    "    z,a=predict(sample,weights)\n",
    "    Z.append(z)\n",
    "    A.append(a)\n",
    "    \n",
    "Z=np.array(Z)\n",
    "A=np.array(A)\n",
    "\n",
    "plt.plot(dataP[:,0],dataP[:,1],'or')\n",
    "plt.plot(dataN[:,0],dataN[:,1],'ob')\n",
    "plt.scatter(XY[Z==0,0],XY[Z==0,1],s=1,c='b',marker='.')\n",
    "plt.scatter(XY[Z==1,0],XY[Z==1,1],s=1,c='r',marker='.')\n",
    "plt.axis([0, 9, -1, 4.5])\n",
    "xmin, xmax, ymin, ymax = plt.axis()\n",
    "#plt.show()\n",
    "\n",
    "# plot the values of the activation values obtained by sampling the observation plane\n",
    "plt.subplot(133) \n",
    "A=np.reshape(A,[xx.size,xy.size])\n",
    "plt.contourf(XX,YY,A, levels=20)\n",
    "plt.plot(dataP[:,0],dataP[:,1],'or')\n",
    "plt.plot(dataN[:,0],dataN[:,1],'ob')\n",
    "plt.plot(xx,xy,'k');\n",
    "plt.axis([xmin, xmax, ymin, ymax])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "- Which of the three representations above is the more informative? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Using Scikit-learn perceptron function\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptronlen(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf = Perceptron(eta0=1,fit_intercept=True, max_iter=1000, n_iter_no_change=5, \\\n",
    "                 random_state=0, shuffle=True, tol=0.001)\n",
    "clf.fit(X,y)\n",
    "w=np.zeros(3)\n",
    "w[0]=clf.intercept_\n",
    "w[1:3]=clf.coef_\n",
    "print(f\"Estimates weights ={w}\")\n",
    "\n",
    "# Visu of decision regions\n",
    "xx=np.linspace(0,8.7,50)\n",
    "yy=np.linspace(-1,8,50)\n",
    "XX,YY= np.meshgrid(xx,yy)\n",
    "XY = np.vstack([XX.flatten(), YY.flatten()]).T\n",
    "\n",
    "Z=clf.predict(XY)\n",
    "\n",
    "plt.plot(dataP[:,0],dataP[:,1],'or')\n",
    "plt.plot(dataN[:,0],dataN[:,1],'ob')\n",
    "plt.scatter(XY[Z==0,0],XY[Z==0,1],s=1,c='b',marker='.')\n",
    "plt.scatter(XY[Z==1,0],XY[Z==1,1],s=1,c='r',marker='.')\n",
    "xmin, xmax, ymin, ymax = plt.axis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "- Using the sklearn reference documentation, identify the role of the parameters \"Shuffle\"\n",
    "- Discuss the interest of introducing such a parameter\n",
    "- What was the value of parameter \"eta0\" in our \"train_weights\" code? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Problem : \n",
    "\n",
    "A two class problem is considered, for concentric spiral classes. \n",
    "The data are synthetized then represented below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "std=.4\n",
    "N=250\n",
    "\n",
    "circ_coord = np.zeros((N, 2))\n",
    "circ_coord[:,0] = 2*np.pi*np.random.rand(N)\n",
    "circ_coord[:int(N/2), 1] = np.abs(.1 + .5*circ_coord[:int(N/2), 0] + std*np.random.randn(int(N/2)))\n",
    "circ_coord[int(N/2):, 1] = np.abs(1 + .8*circ_coord[int(N/2):, 0] + std*np.random.randn(int(N/2)))\n",
    "\n",
    "X = np.zeros((N,2))\n",
    "X[:,0] = circ_coord[:,1]*np.sin(circ_coord[:,0])\n",
    "X[:,1] = circ_coord[:,1]*np.cos(circ_coord[:,0])\n",
    "\n",
    "label = np.zeros(N)\n",
    "label[:int(N/2)]=1\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1],c=label,s=10)\n",
    "plt.axis('square')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "- Do you think that a perceptron is a performant classifier for this problem?\n",
    "\n",
    "Learning the perceptron from the learning set (X,y) and visulizing the decision region is performed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(eta0=1,fit_intercept=True, max_iter=10000, n_iter_no_change=5, \\\n",
    "                 random_state=0, shuffle=True, tol=0.001, \\\n",
    "                 validation_fraction=0.1,warm_start=True)\n",
    "clf.fit(X,label)\n",
    "\n",
    "# Visu of decision regions\n",
    "xx=np.linspace(-6,6,100)\n",
    "yy=np.linspace(-6,6,100)\n",
    "XX,YY= np.meshgrid(xx,yy)\n",
    "XY = np.vstack([XX.flatten(), YY.flatten()]).T\n",
    "    \n",
    "Z=clf.predict(XY)   \n",
    "plt.scatter(XY[Z==0,0],XY[Z==0,1],s=1,c='b',marker='.')\n",
    "plt.scatter(XY[Z==1,0],XY[Z==1,1],s=1,c='y',marker='.')\n",
    "plt.scatter(X[:,0],X[:,1],c=label,s=10)\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Propose a coordinate change that would make the problem linearly separable. What do you conclude about the choice of the representation space of the analyzed data? Display the new classification frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_circ_coord(X):\n",
    "    res = np.zeros((X.shape[0],2))\n",
    "    # TODO: new coordinate\n",
    "    return res\n",
    "newX = to_circ_coord(X)\n",
    "plt.scatter(newX[:,0], newX[:,1],c=label,s=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
