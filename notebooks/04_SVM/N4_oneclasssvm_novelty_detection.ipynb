{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/05_SVM/N4_oneclasssvm_novelty_detection.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-class SVM with non-linear kernel (RBF) for novelty detection.\n",
    "\n",
    "One-class SVM is an *unsupervised* algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novelty Detection\n",
    "*Quoting scikit-learn user guide*\n",
    "\n",
    "Consider a data set of $n$ observations from the same distribution described by $p$ features. Consider now that we add one more observation to that data set. Is the new observation so different from the others that we can doubt it is regular? (i.e. does it come from the same distribution?) Or on the contrary, is it so similar to the other that we cannot distinguish it from the original observations? This is the question addressed by the novelty detection tools and methods.\n",
    "\n",
    "In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding $p$-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment.\n",
    "\n",
    "The One-Class SVM has been introduced in \n",
    "> [Estimating the support of a high-dimensional distribution](https://dl.acm.org/citation.cfm?id=1119749), Schölkopf, Bernhard, *et al*. Neural computation 13.7 (2001): 1443-1471,\n",
    "\n",
    "for that purpose and implemented in the Support Vector Machines module in the `svm.OneClassSVM` object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The $\\nu$ parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-class SVM: Mathematical Formulation\n",
    "\n",
    "SVM based one-class classification relies on identifying the smallest *hypersphere* (with radius r, and center c) consisting of all the data points. Formally, the problem can be defined in the following constrained optimization form,\n",
    "\n",
    "$$\\min_{r,c} r^2 \\text{ subject to, } ||\\Phi(x_i) - c||^2 \\le r^2 \\;\\; \\forall i = 1, 2, \\ldots, n$$\n",
    "where $\\Phi(\\cdot)$ is a feature expansion function.\n",
    "\n",
    "However, the above formulation is highly restrictive, and is sensitive to the presence of outliers. Therefore, a flexible formulation, that allow for the presence of outliers is formulated as shown below,\n",
    "\n",
    "$$\\min_{r,c,\\zeta} r^2 + \\frac{1}{\\nu n}\\sum_{i=1}^{n}\\zeta_i$$\n",
    "$$\\text{subject to, } ||\\Phi(x_i) - c||^2 \\le r^2 + \\zeta_i \\;\\; \\forall i = 1, 2, \\ldots, n$$\n",
    "\n",
    "From Karush-Kuhn-Tucker (KKT) optimality conditions, we get\n",
    "\n",
    "$$c = \\sum_{i=1}^{n}\\alpha_i\\Phi(x_i),$$\n",
    "\n",
    "where the Lagrange multiplier $\\alpha_i$'s are the solution to the following optimization problem:\n",
    "\n",
    "$$\\max_\\alpha \\sum_{i=1}^{n}\\alpha_i k(x_i, x_i) - \\sum_{i, j = 1}^{n}\\alpha_i\\alpha_j k(x_i, x_j)$$\n",
    "\n",
    "subject to, $\\sum_{i=1}^{n}\\alpha_i = 1 \\text{ and } 0 \\le \\alpha_i \\le \\frac{1}{\\nu n} \\text{for all } i = 1,2,...,n$, where \n",
    "$k(\\cdot,\\cdot)$  is the kernel function from which $\\Phi()$ can be derived.\n",
    "\n",
    "The introduction of kernel function $k(\\cdot,\\cdot)$ provide additional flexibility to the one-class support vector machine algorithm w.r.t. a rigid spherical frontier in the original data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the data sets: \n",
    "- training set with *regular* observations\n",
    "- test set with both *regular* and *abnormal* observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.RandomState(1)\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "\n",
    "# Generate bimodal train data\n",
    "n_train = 200 \n",
    "X = 0.3 * np.random.randn(n_train//2, 2)\n",
    "X_train = np.r_[X + 2, X - 2]\n",
    "# Generate some regular novel observations\n",
    "n_test = 100\n",
    "X = 0.3 * np.random.randn(n_test//2, 2)\n",
    "X_test = np.r_[X + 2, X - 2]\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(n_test//2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the one-class model for given specified proportion of outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.svm as svm\n",
    "\n",
    "# Set the proba of finding a new, but regular, observation outside the frontier\n",
    "nu = .1\n",
    "# fit the model\n",
    "clf = svm.OneClassSVM(nu=nu, kernel=\"rbf\", gamma=.1)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# Compute prediction errors for novelty detection\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "\n",
    "n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the train + test datasets and the learned frontier to detect *novelties*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,9))\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"Novelty Detection\", fontsize=14)\n",
    "plt.contour(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=3, colors='darkred')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "\n",
    "s = 40\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s,\n",
    "                 edgecolors='k')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s,\n",
    "                edgecolors='k')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([a.collections[0], b1, b2, c],\n",
    "           [\"learned frontier\", \"training observations\",\n",
    "            \"new (test) regular observations\", \"new (test) abnormal observations\"],\n",
    "           loc=\"upper left\",\n",
    "           fontsize=14)\n",
    "plt.xlabel(\n",
    "    \"error train: {}/{} ; errors novel regular (false alarms): {}/{} ; \"\n",
    "    \"errors novel abnormal (miss-detections): {}/{}\".format\n",
    "    (n_error_train, n_train, n_error_test, n_test, \n",
    "     n_error_outliers, n_test), \n",
    "    fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "- Is this a supervised algorithm (explain why)?\n",
    "- Gradually increase or decreases the probability $\\nu$ in the range $(0,1)$. How does this influence the detection performances for both new *regular* and *abnormal* (novelty) samples?\n",
    "- Are the results very sensitive to the choice of the gamma parameter of the RBF kernel (test different values), and justify? Is it possible to cross-validate these hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
