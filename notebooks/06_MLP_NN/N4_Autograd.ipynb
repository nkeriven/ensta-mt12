{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e08125c",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/07_MLP_NN/N4_Autograd.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123360a9",
   "metadata": {},
   "source": [
    "We define a simple quadratic function for which we know the gradient, and test torch autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf803ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pd matrix Sigma\n",
    "Sigma = torch.randn(5,5)\n",
    "Sigma = Sigma @ Sigma.t() # we obtain a symmetric pd matrix\n",
    "\n",
    "x = torch.rand(5, requires_grad=True)\n",
    "\n",
    "value = x[None,:] @ Sigma @ x[:,None]\n",
    "value.backward() # the magic happen here !\n",
    "print(f'gradient with autograd {x.grad}')\n",
    "print(f'Gradient with math {2*Sigma @x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50720e97",
   "metadata": {},
   "source": [
    "### Exercize\n",
    "\n",
    "Test a few other functions with autograd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309adb27",
   "metadata": {},
   "source": [
    "### Neural nets\n",
    "\n",
    "We test simple neural nets on MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/zip_train_full.mat -O zip_train_full.mat\n",
    "!wget https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/zip_test_full.mat -O zip_test_full.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "# Warning: put the data files in the notebook directory\n",
    "data = spio.loadmat(\"zip_train_full.mat\")\n",
    "Xtrain = data[\"Xtrain_full\"]\n",
    "Ytrain = data[\"Ytrain_full\"]\n",
    "Xshape = Xtrain.shape\n",
    "Ytrain = np.reshape(Ytrain, (Xshape[0],))\n",
    "Yshape = Ytrain.shape\n",
    "\n",
    "print(\"Xtrain is (n={},p={}) sized\".format(Xshape[0], Xshape[1]))\n",
    "print(\"Ytrain is a (n={},) sized vector of reponses\".format(Yshape[0]))\n",
    "\n",
    "data_test = spio.loadmat(\"zip_test_full.mat\")\n",
    "Xtest = data_test[\"Xtest_full\"]\n",
    "Ytest = data_test[\"Ytest_full\"]\n",
    "Ytest = np.reshape(Ytest, (Xtest.shape[0],))\n",
    "print(\"Xtest is (n={},p={}) sized\".format(Xtest.shape[0], Xtest.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a5c305",
   "metadata": {},
   "source": [
    "#### Dataloader\n",
    "Pytorch offers a class to automatically handle datasets and batch division, etc., the class `Dataloader`. Its input is a list of lists `[x, y]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "temp = []\n",
    "for i in range(len(Xtrain)):\n",
    "   temp.append([Xtrain[i], Ytrain[i]])\n",
    "\n",
    "trainloader = DataLoader(temp, shuffle=True, batch_size=32)\n",
    "\n",
    "temp = []\n",
    "for i in range(len(Xtest)):\n",
    "   temp.append([Xtest[i], Ytest[i]])\n",
    "\n",
    "testloader = DataLoader(temp, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06bfea",
   "metadata": {},
   "source": [
    "#### Module\n",
    "The main superclass to define a Neural Net is called `Module`. The main function it has to implement is the function `forward`, which computes the output of the neural net on `x`. **The first dimension of `x` is necessarily the batch size**!\n",
    "\n",
    "Neurals Nets are formed of layers, often pre-implemented in Pytorch, themselves Modules. See the doc.\n",
    "\n",
    "For a Multilayer perceptron, we consider only dense Linear layers, whose arguments are the input and output dimension. Let's consider 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2aff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim=30):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(256, hidden_dim) # implicitely, all parameters\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each layers has\n",
    "\n",
    "print(net.lin1.weight) # of class Parameter, contains a tensor with requires_grad=True.\n",
    "#All together, form the parameters of the Net\n",
    "print(net.parameters)\n",
    "print(list(net.parameters())[0]) # net.parameters() is a generator, like range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1c148",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "In pytorch, gradient descent are handled by `Optimizer`, like SGD, Adam, RMSProp, etc. They take a neural net as input, to optimizer over its net.parameters() that have requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e38ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2495b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's optimize !\n",
    "import time\n",
    "start=time.time()\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in trainloader:\n",
    "        # a dataloader is a generator as well. When enumerated over, it yields batches one after the other\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].float(), data[1]\n",
    "\n",
    "        # zero the parameter gradients. DO NOT FORGET THIS !!!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs) # compute the network forward\n",
    "        loss = criterion(outputs, labels) # compute the loss\n",
    "        loss.backward() # autograd: each parameter contains its gradient\n",
    "        optimizer.step() # take a gradient step\n",
    "        total_loss+=loss.item() # item takes only the value\n",
    "\n",
    "    print(f'[Epoch {epoch + 1} loss: {total_loss / Xtrain.shape[0]}')\n",
    "\n",
    "print(time.time()-start)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy over test set\n",
    "acc=0\n",
    "for data in testloader:\n",
    "    # a dataloader is a generator as well. When enumerated over, it yields batches one after the other\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data[0].float(), data[1]\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs) # compute the network forward\n",
    "    outputs = outputs.argmax(axis=1)\n",
    "    acc += (outputs==labels).sum()\n",
    "print(f'Accuracy {acc/Xtest.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70840fed",
   "metadata": {},
   "source": [
    "Draw a training error/test error curves vs. number of epochs. Run for many epochs. What do you observe ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
