{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/02a_discriminant_analysis/N2_discriminant-analysis_zip_digits.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten digits recognition\n",
    "\n",
    "We aim to recognize handwritten digits in digital images.\n",
    "After normalization (see [here](http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.info.txt) for more details on these data, which are taken from zip codes on US postal envelopes), the resulting images are composed of $16 \\times 16$ pixels, each pixel being quantified in gray levels in the interval $[- 1.1]$.\n",
    "Thus, we  have $X \\in [-1,1]^{256}$, i.e $d = 256$, and $Y \\in \\{0,1, \\ldots, 9 \\}$.\n",
    "\n",
    "## Part I\n",
    "\n",
    "In a fisrt time, we have a training set of $n=257$ samples  (file `zip_train.mat`),  and a test set  of $255$ samples (file `zip_test.mat`).\n",
    "The code cells below allow to\n",
    "- [Display some images from each class](#Display-some-digits)\n",
    "- [Compare several discriminant analysis models and perform a Linear Discriminant Analysis on the training set under the Naïve Bayes assumption (diagonal covariance matrix)](#Compare-the-performances-obtained-for-different-variants-of-discriminant-analysis)\n",
    "- [Display as an image the estimated mean values for each class $k=0, \\ldots, 9$](#Display-as-an-image-the-estimated-mean-values-for-each-class)\n",
    "- [Display some image realizations according to the generative model that has been learned](#Display-some-image-realizations-according-to-the-generative-model-that-has-been-learned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data sets\n",
    "\n",
    "You can either download the data yourself and place it in the same folder as the notebook, or use the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/zip_train.mat -O zip_train.mat\n",
    "!wget https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/zip_test.mat -O zip_test.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "# Warning: put the data files in the notebook directory\n",
    "data = spio.loadmat(\"zip_train.mat\")\n",
    "Xtrain = data[\"Xtrain\"]\n",
    "Ytrain = data[\"Ytrain\"]\n",
    "Xshape = Xtrain.shape\n",
    "Ytrain = np.reshape(Ytrain, (Xshape[0],))\n",
    "Yshape = Ytrain.shape\n",
    "\n",
    "print(f\"Xtrain is (n={Xshape[0]},p={Xshape[1]}) sized\")\n",
    "print(f\"Ytrain is a (n={Yshape[0]},) sized vector of reponses\")\n",
    "\n",
    "data_test = spio.loadmat(\"zip_test.mat\")\n",
    "Xtest = data_test[\"Xtest\"]\n",
    "Ytest = data_test[\"Ytest\"]\n",
    "Ytest = np.reshape(Ytest, (Xtest.shape[0],))\n",
    "print(f\"Xtest is (n={Xtest.shape[0]},p={Xtest.shape[1]}) sized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))  # to specify the size of the images\n",
    "for i in range(10):\n",
    "    fig.add_subplot(2, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    mplot = plt.imshow(np.reshape(Xtrain[i], (16, 16)), cmap=\"gray_r\")\n",
    "    mplot.axes.get_xaxis().set_visible(False)\n",
    "    mplot.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the performances obtained for different variants of discriminant analysis\n",
    "\n",
    "- We already know that LDA and QDA methods are already coded in scikit-learn through [LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis) and  [QuadraticDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis).\n",
    "- Moreover, QDA with Naïve Bayes is available with [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "- Last, LDA with Naïve Bayes can be obtained as a special case of the [LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis) method when the shrinkage parameter equals 1 (`shrinkage=1`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.naive_bayes import GaussianNB as QDA_NB\n",
    "\n",
    "\n",
    "def print_discr_analysis_perf(model, msg):\n",
    "    # train the discr_analysis model\n",
    "    model.fit(Xtrain, Ytrain)\n",
    "    # compute the train/test misclassification rates (mcr)\n",
    "    Ytrain_hat = model.predict(Xtrain)\n",
    "    mcr_train = np.mean(Ytrain_hat != Ytrain)\n",
    "    Ytest_hat = model.predict(Xtest)\n",
    "    mcr_test = np.mean(Ytest_hat != Ytest)\n",
    "    print(\"{:7s} misclassification rates: train = {:0.3f}, test = {:0.3f}\".\n",
    "          format(msg, mcr_train, mcr_test))\n",
    "\n",
    "\n",
    "# QDA case\n",
    "print_discr_analysis_perf(QDA(), 'QDA')\n",
    "# QDA + NB case\n",
    "print_discr_analysis_perf(QDA_NB(), 'QDA+NB')\n",
    "# LDA case\n",
    "print_discr_analysis_perf(LDA(), 'LDA')\n",
    "# LDA + NB case\n",
    "print_discr_analysis_perf(LDA(solver=\"eigen\", shrinkage=1), 'LDA+NB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise I.1:\n",
    "- Can you explain why sklearn algo raises a warning when we train the QDA and LDA models (and not in the QDA+NB and LDA+NB case)?\n",
    "- How to justify that the linear discriminant analysis under the Naïve Bayes assumption seems the most appropriate among all the methods of discriminant analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display as an image the estimated mean values for each class \n",
    "Wee focus now on the LDA with *NB assumption* (i.e., same *diagonal* covariance matrix for all the classes) classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA + NB\n",
    "lda_nb = LDA(solver=\"eigen\",  shrinkage=1)\n",
    "lda_nb.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this show the mean vector of the LDA classes\n",
    "fig = plt.figure(figsize=(12, 5))  # to specify the size of the images\n",
    "for k in range(0, 10):\n",
    "    fig.add_subplot(2, 5, k+1)\n",
    "    Xmean = lda_nb.means_[k]\n",
    "    mplot = plt.imshow(np.reshape(Xmean, (16, 16)), cmap=\"gray_r\")\n",
    "    plt.title(k)\n",
    "    # hide the axis\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some image realizations according to the generative model that has been learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Draw some Gaussian vectors according to the LDA with NB model')\n",
    "fig = plt.figure(figsize=(12, 5))  # to specify the size of the images\n",
    "for k in range(0, 10):\n",
    "    fig.add_subplot(2, 5, k+1)\n",
    "    # generate the gaussian vector\n",
    "    Xsynth = lda_nb.means_[k] + \\\n",
    "        np.real(sp.linalg.sqrtm(lda_nb.covariance_)) @ np.random.randn(256)\n",
    "    mplot = plt.imshow(np.reshape(Xsynth, (16, 16)), cmap=\"gray_r\")\n",
    "    plt.title(k)\n",
    "    # hide the axis\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise I.2:\n",
    "Run several times the cells above and below to observe different random realizations\n",
    "- Do these synthetic examples seem realistic to you? \n",
    "- What is the interest of such a model?\n",
    "- Comparing with QDA based synthetic examples on the cell below, what can we conclude (remember that QDA obtains here catastrophic generalization performances on test data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic examples for QDA: display some image realizations based on the QDA generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda = QDA(store_covariance=True)\n",
    "qda.fit(Xtrain, Ytrain)\n",
    "\n",
    "print('Draw some Gaussian vectors according to the QDA model')\n",
    "fig = plt.figure(figsize=(12, 5))  # to specify the size of the images\n",
    "for k in range(0, 10):\n",
    "    fig.add_subplot(2, 5, k+1)\n",
    "    # generate the gaussian vector\n",
    "    Xsynth = qda.means_[k] + \\\n",
    "        np.real(sp.linalg.sqrtm(qda.covariance_[k])) @ np.random.randn(256)\n",
    "    mplot = plt.imshow(np.reshape(Xsynth, (16, 16)), cmap=\"gray_r\")\n",
    "    plt.title(k)\n",
    "    # hide the axis\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Regularized discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain more flexibility for LDA methods, a common procedure is regularized (linear) discriminant analysis:  all classes have the same covariance matrix\n",
    "\n",
    "$\n",
    "\\hat{\\Sigma}_\\gamma = (1-\\gamma) \\hat{\\Sigma} + \\gamma \\mathrm{diag}{(\\hat{\\Sigma})},\n",
    "$\n",
    "\n",
    "where $\\hat{\\Sigma}$ is the empirical pooled covariance matrix, $\\mathrm{diag}{(\\hat{\\Sigma})}$ is the diagonal matrix with diagonal entries equal to those of $\\hat{\\Sigma}$ (Naïve Bayes empirical estimate), and $\\gamma \\in [0,1]$ is the amount of regularization.\n",
    "\n",
    "Within sklearn, the regularization coefficient $\\gamma$ correponds to the `shrinkage` parameter of    [LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis).\n",
    "\n",
    "\n",
    "The code cells below allow to\n",
    "- [Display misclassification rate curves as a function of the regularizion coefficient](#Display-misclassification-rate-curves-as-a-function-of-the-regularized-discriminant-analysis-coefficient-%24%5Cgamma%24)\n",
    "- [Estimate the optimal parameter $\\gamma$ in an automatic way](#Automatic-estimation-of-the-optimal-regularization-parameter-$\\gamma$)\n",
    "- [Train LDA on a larger dataset](#Dealing-with-larger-datasets)\n",
    "- [Display the confusion matrix to see the most common errors](#Display-the-confusion-matrix)\n",
    "\n",
    "\n",
    "### Display misclassification rate curves as a function of the regularized discriminant analysis coefficient $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 30  # you can crank up the resolution but it will not change a lot\n",
    "gamma = np.linspace(1e-6, 1, res) # array of regularization coefficients.\n",
    "# 0 results in numerical issue, we take a small minimum value\n",
    "\n",
    "#### EXO: display test and train error vs gamma. Give the optimal value wrt test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.1: \n",
    "- Explain what happens in the limiting case $\\gamma=0$ or $\\gamma=1$? And which methods correspond to these particular cases of regularized discriminant analysis?\n",
    "- What are a good choices here for the $\\gamma$ values?\n",
    "- In practice, when there is no test set, which common procedure can we use to estimate the optimal value of $\\gamma$? (note: we will see below a performant and cost-efficient alternative)\n",
    "- (*Optional*) Compare with the performance/computational cost  obtained for a k-NN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic estimation of the optimal regularization parameter $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the high dimensional framework, i.e. large dimension $p$ and large sample size $n$, this becomes possible to estimate the optimal shrinkage parameter (which minimizes the Frobenius norm $||\\Sigma-\\widehat{\\Sigma}_\\gamma||$) in a **simple analytic way** following the result introduced in\n",
    " > Ledoit O, Wolf M. *Honey, I Shrunk the Sample Covariance Matrix*. The Journal of Portfolio Management 30(4), 110-119, 2004.\n",
    "\n",
    "With sklearn, this can be used by setting the shrinkage parameter of the `LinearDiscriminantAnalysis` class to ‘auto’, i.e. ```shrinkage='auto'```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_auto = LDA(solver=\"eigen\", shrinkage=\"auto\")\n",
    "lda_auto.fit(Xtrain, Ytrain)\n",
    "\n",
    "y_hat = lda_auto.predict(Xtrain)\n",
    "mcr_train = np.mean(Ytrain != y_hat)\n",
    "\n",
    "y_hat = lda_auto.predict(Xtest)\n",
    "mcr_test = np.mean(Ytest != y_hat)\n",
    "\n",
    "print(\"Auto regularized LDA mcr: train = {:0.3f}, test = {:0.3f}\".\n",
    "          format(mcr_train, mcr_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.2:\n",
    "- Is this in good agreement with the estimates of the optimal regularized LDA perfomance derived previously?\n",
    "- What are the benefits of this 'automatic' method compare to cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with larger datasets\n",
    "We consider now the larger datasets `Xtrain_full`  (Matlab file `zip_train_full.mat`) and `Xtest_full` (`zip_test_full.mat`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/zip_train_full.mat -O zip_train_full.mat\n",
    "!wget https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/zip_test_full.mat -O zip_test_full.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: put the data files in the notebook directory\n",
    "data = spio.loadmat(\"zip_train_full.mat\")\n",
    "Xtrain_full = data[\"Xtrain_full\"]\n",
    "Ytrain_full = data[\"Ytrain_full\"]\n",
    "Xshape = Xtrain_full.shape\n",
    "Ytrain_full = np.reshape(Ytrain_full, (Xshape[0],))\n",
    "Yshape = Ytrain_full.shape\n",
    "\n",
    "print(\"Xtrain_full is (n={},p={}) sized\".format(Xshape[0], Xshape[1]))\n",
    "print(\"Ytrain_full is a (n={},) sized vector of reponses\".format(Yshape[0]))\n",
    "\n",
    "data_test = spio.loadmat(\"zip_test_full.mat\")\n",
    "Xtest_full = data_test[\"Xtest_full\"]\n",
    "Ytest_full = data_test[\"Ytest_full\"]\n",
    "Ytest_full = np.reshape(Ytest_full, (Xtest_full.shape[0],))\n",
    "print(\"Xtest is (n={},p={}) sized\".format(Xtest_full.shape[0], Xtest_full.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.3:\n",
    "- Apply all the previous algo to these larger datasets and compare the performances with the previous one. What can you conclude? Do we still get numerical warnings ? why ?\n",
    "- Compare now the optimal values for the regularization parameter $\\gamma$ for LDA. How to explain this?\n",
    "- (*Optional*) Compare now with a regularized QDA (set the `reg_param` parameter). How to explain this?\n",
    "- (*Optional*) Compare with the the performance/cost ratio obtained for a k-NN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the *confusion matrix*\n",
    "\n",
    "The *confusion matrix* (see the [scikitlearn user guide](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) for some examples) is a useful tool in supervised learning to evaluate classification accuracy. Quoting Wikipedia:\n",
    ">Each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class (or vice versa, depending on the convention). The name stems from the fact that it makes it easy to see the confusion between two classes (i.e. commonly mislabeling one as another). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "lda_auto = LDA(solver=\"eigen\", shrinkage=\"auto\")\n",
    "lda_auto.fit(Xtrain_full, Ytrain_full)\n",
    "\n",
    "y_hat = lda_auto.predict(Xtest_full)\n",
    "confusion_matrix(Ytest_full, y_hat) # y_hat is the auto regularized LDA prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(lda_auto, Xtest_full, Ytest_full)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise II.4:\n",
    "- What are the the most common confusions between classes?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
