{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/02_PCA/N2_pca_versus_lda.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare two transformation method on the IRIS data set: \n",
    "\n",
    "  - Principal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. Here we plot the different samples on the 2 first principal components.\n",
    "\n",
    "  - Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance *between classes*. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and standardize the data\n",
    "iris = load_iris()\n",
    "X = iris.data ;  y = iris.target \n",
    "# load_iris(return_X_y=True)\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot some pairs of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2,2,figsize=(12, 12))\n",
    "# Axe 1 and 2\n",
    "axes[0,0].scatter(X[:,0], X[:,1],c=iris.target,)\n",
    "axes[0,0].set_xlabel(iris.feature_names[0])\n",
    "axes[0,0].set_ylabel(iris.feature_names[1])\n",
    "\n",
    "# Axe 1 and 3\n",
    "axes[0,1].scatter(X[:,0], X[:,2],c=iris.target,)\n",
    "axes[0,1].set_xlabel(iris.feature_names[0])\n",
    "axes[0,1].set_ylabel(iris.feature_names[2])\n",
    "\n",
    "# Axe 2 and 4\n",
    "axes[1,0].scatter(X[:,1], X[:,3],c=iris.target,)\n",
    "axes[1,0].set_xlabel(iris.feature_names[1])\n",
    "axes[1,0].set_ylabel(iris.feature_names[3])\n",
    "\n",
    "# Axe 3 and 4\n",
    "axes[1,1].scatter(X[:,2], X[:,3],c=iris.target)\n",
    "axes[1,1].set_xlabel(iris.feature_names[2])\n",
    "axes[1,1].set_ylabel(iris.feature_names[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "We apply  PCA transformation to the dataset, and plot the cumulative variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "l = pca.explained_variance_ratio_\n",
    "\n",
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "PC_labels = ['PC' + str(nb+1) for nb in range(pca.n_components_)] \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(PC_values, l.cumsum(), linewidth=2, edgecolor='k')\n",
    "plt.xticks(ticks=PC_values, labels=PC_labels)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can project the data on the first two PCs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp = pca.transform(X) # linear projection along the axes that maximize the dispersion (variance) \n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(Xp[:,0], Xp[:,1],c=y) # plot the 2 first components\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "\n",
    "LDA can also be used for dimensionality reduction, to a dimension $L\\leq K-1$ where $K$ is the number of classes. Indeed, after computing the $K$ means of each Gaussian, they define a $K-1$ affine subspace, onto which we can project the data. In this subspace, we can again compute the directions of most variance (through a spectral method).\n",
    "See more at https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-lda-dimensionality-reduction\n",
    "\n",
    "We apply now  LDA transformation to find the directions (vectors) that maximizes the class separation, and plot their cumulative variance ratio to explain the Fisher linear discriminant separation criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA()\n",
    "lda.fit(X, y)\n",
    "l = lda.explained_variance_ratio_\n",
    "\n",
    "\n",
    "LDA_values = np.arange(2) + 1\n",
    "LDA_labels = ['LDA' + str(nb+1) for nb in range(2)] \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(LDA_values, l.cumsum(), linewidth=2, edgecolor='k')\n",
    "plt.xticks(ticks=LDA_values, labels=LDA_labels)\n",
    "plt.title('Fisher linear analysis (LDA)')\n",
    "plt.xlabel('LDA components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in LDA we assume that all classes have the same estimated covariance. Thus we can rescale the data so that this covariance is the identity. Then one can show that to classify a data point after scaling is equivalent to finding the estimated class mean which is closest to the data point in the Euclidean distance (see course). But this can be done just as well after projecting on the affine subspace generated by all the (rescaled) means for all classes. \n",
    "\n",
    "This shows that, implicit in the LDA classifier, there is a *dimensionality reduction by linear projection onto a $K-1$ dimensional space*, where $K$ is the total number of target classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp = lda.transform(X) # linear projection to maximize class separation for the fitted LDA model \n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(Xp[:,0], Xp[:,1],c=y) # plot the 2 LDA vectors\n",
    "ax.set_xlabel('LDA1')\n",
    "ax.set_ylabel('LDA2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Does the PCA requires the class label to transform the dataset? \n",
    "- How many PCA principal component seems sufficient to mostly  explain the variance of this dataset?\n",
    "- How many PCA principal component seems sufficient to get a quite accurate separation between the classes?\n",
    "- Explain why the explained variance ratio is 1 for the LDA with two components. \n",
    "- Does LDA allows us to get an accurate separation between the classes when using only the first LDA vector?\n",
    "- Do you think that PCA can be useful to transform this dataset for visualization or dimension reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
