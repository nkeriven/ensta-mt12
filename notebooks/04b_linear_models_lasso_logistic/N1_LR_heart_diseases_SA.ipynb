{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/04b_linear_models_lasso_logistic/N1_LR_heart_diseases_SA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# South Africa Heart Diseases Data\n",
    "A retrospective sample of males in a heart-disease high-risk region\n",
    "of the Western Cape, South Africa. There are roughly two controls per\n",
    "case of Coranary Heart Disease (CHD). Many of the CHD positive men have undergone blood\n",
    "pressure reduction treatment and other programs to reduce their risk\n",
    "factors after their CHD event. In some cases the measurements were\n",
    "made after these treatments. These data are taken from a larger\n",
    "dataset, described in Rousseauw et al, 1983, South African Medical Journal. \n",
    "\n",
    "| Variable name     | Description              |\n",
    "|:---------|:-------------------------|\n",
    "| sbp\t  |\tsystolic blood **pressure** |\n",
    "| tobacco\t|\tcumulative **tobacco** (kg) |\n",
    "| ldl |\t\tlow densiity lipoprotein **cholesterol** |\n",
    "| adiposity |                                     |\n",
    "| famhist\t |\t**family** history of heart disease (Present=1, Absent=0) |\n",
    "| typea\t|\ttype-A **behavior** |\n",
    "| obesity |            |\n",
    "| alcohol\t|\tcurrent **alcohol** consumption |\n",
    "| age\t\t| **age** at onset |\n",
    "| chd\t\t| **response**: coronary heart disease  (Present=1, Absent=0) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#load data set\n",
    "heart = pd.read_csv('https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/SAheart.csv', sep=',', header=0)\n",
    "heart = heart.replace(to_replace='Present', value=1)\n",
    "heart = heart.replace(to_replace='Absent', value=0)\n",
    "heart.head() #data overview: variable names and first values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format the input/target data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray( heart.iloc[:,10], 'float' )\n",
    "X = np.asarray( heart.iloc[:,1:10], 'float' )\n",
    "names = list(heart.columns[1:10]) # variable names\n",
    "y = heart.iloc[:,10]\n",
    "X = heart.iloc[:,1:10]\n",
    "\n",
    "# Data matrix X is (n,p) where p is the number of variable and n the number of sample\n",
    "(n,p) = X.shape\n",
    "ncases = int(np.sum(y)) # number of cases\n",
    "print('There is {} samples: {} cases and {:d} control'.format(n,ncases,n-ncases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ordinary (without regularization) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "# Standardize the data (useful to compare the variable weights)\n",
    "sc = StandardScaler()\n",
    "X = heart.iloc[:,1:10]\n",
    "Xs= sc.fit_transform(X)\n",
    "\n",
    "print(\"Computing LR estimates ...\")\n",
    "start = time()\n",
    "clf = LogisticRegression(penalty='none', tol=1e-6, max_iter=int(1e6))\n",
    "clf.fit(Xs, y)\n",
    "print(\"This took %0.3fs\" % (time() - start))\n",
    "betas = pd.DataFrame.from_records(clf.coef_, columns=names, index=['Weights'])\n",
    "betas['intercept'] = clf.intercept_\n",
    "print('Estimated Weights for standardized variables:\\n')\n",
    "betas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "- what does a positive, negative or near-zero weight mean to predict heart disease?\n",
    "- How do you interpret the weight of obesity for instance?\n",
    "- How can you explain such surprising findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\ell_1$ penalized  Logistic Regression and lasso path\n",
    "\n",
    "Regularization functions such as $\\ell_1$ or $\\ell_2$ penalty (or combined $\\ell_1$/$\\ell_2$ penalty as in *elastic net*)  can also be used in generalized linear model such that Logistic Regression. The residual sum of squares criterion used for linear regression is then replaced by the opposite of the (conditional) log likelihood. \n",
    "For instance for Logitic Regression with Lasso-type regularization ($\\ell_1$ penalty), this yields for binary classification $y_i \\in \\{-1,+1\\}$ the following optimization problem:\n",
    "$$\n",
    "\\min_{\\beta}   \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T \\beta )) + 1) +  \\lambda \\| \\beta \\|_1.\n",
    "$$\n",
    "Within scikit-learn, penalized Logistic Regression is available through the `penalty` parameter of [`linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.htm),  parameter `C` being the inverse of regularization strength such that $C=\\frac{1}{\\lambda}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute lasso path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "from sklearn.svm import l1_min_c\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standardize the data! \n",
    "# Don't forget that scale matters for penaliezd regression such that Lasso or ridge\n",
    "# (This is true also for distance based methods s.t. K-NN).\n",
    "# If one variable has a larger magnitude than the other (imagine that you change the \n",
    "# unity for a variable from kilograms to grams), this variable will be much less shrunken than \n",
    "# the others.\n",
    "# Advice: Except if you have a good reason to do the opposite, standardize all your variables \n",
    "# to be sure that they are comparable\n",
    "sc = StandardScaler()\n",
    "Xs= sc.fit_transform(X) #center  (zero mean) and reduce (unit variance) the variables \n",
    "\n",
    "# generate useful values for the regularization parameter (log-scale)\n",
    "cs = l1_min_c(Xs, y, loss='log') * np.logspace(-1, 4, 30)\n",
    "\n",
    "# Use a stochastic gradient descent \n",
    "print(\"Computing regularization path ...\")\n",
    "start = time()\n",
    "clf = LogisticRegression(penalty='l1', solver='saga',\n",
    "                                      tol=1e-6, max_iter=int(1e6),\n",
    "                                      warm_start=True)\n",
    "coefs_ = []\n",
    "beta_l1norm = []\n",
    "for c in cs:\n",
    "    clf.set_params(C=c)\n",
    "    clf.fit(Xs, y)\n",
    "    beta_l1norm.append( np.sum(np.abs(clf.coef_.ravel()))) \n",
    "    coefs_.append(clf.coef_.ravel().copy())\n",
    "print(\"This took %0.3fs\" % (time() - start))\n",
    "\n",
    "betas = np.array(coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display lasso path Vs l1 norm of the coeff vector\n",
    "plt.figure(figsize=(12,6))\n",
    "#plt.plot(np.log10(cs), coefs_, marker='o')\n",
    "plt.semilogx(1/cs, betas, marker='o')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Logistic Regression Path')\n",
    "plt.axis('tight')\n",
    "plt.legend(names, fontsize=14)\n",
    "plt.grid('On')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation of the LR model with Lasso penalty\n",
    "\n",
    "We estimate now the optimal regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "print(\"Computing K-fold CV ...\")\n",
    "# K fold cross validation  (K=5)\n",
    "start = time()\n",
    "cs = l1_min_c(Xs, y, loss='log') * np.logspace(0, 2, 50) # the vector fot the alpha (lasso penalty parameter) values\n",
    "model = LogisticRegressionCV(Cs=cs, cv=5, penalty='l1', solver='saga', tol=1e-6).fit(Xs,y)\n",
    "print(\"This took %0.3fs\" % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now model is tuned with the penalty parameter estimated by CV\n",
    "lambda_cv = 1/model.C_[0]\n",
    "# The coef estimated with CV\n",
    "beta_l1norm = np.sum(np.abs(model.coef_))\n",
    "\n",
    "print('CV estimates:')\n",
    "print('- lambda = {:.3f}, which yields ||beta||_1 = {:.3f}\\n'.format(lambda_cv,beta_l1norm) )\n",
    "print('CV weights for standardized variables:')\n",
    "betas_cv = pd.DataFrame.from_records(model.coef_, columns=names, index=['Weights'])\n",
    "betas_cv['intercept'] = clf.intercept_\n",
    "betas_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- What are the only significant variables estimated with cross-validation? \n",
    "- How can we rank them by significance order (*hint: look at the lasso path*)?\n",
    "- Do these results seem more credible (than those obtain without regularization) to predict heart diseases? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the predicted CHD probability for some patients\n",
    "\n",
    "For this kind of problem, we are more of course more interested in the modeling between the inputs/response variables and their interpretation rather than the only prediction of the binary responses  (CHD or not). With a generalized linear model such that LR, it is possible to compute a risk probability for each patient and to assess and to interpret the influence of each variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted risk for the first patient\n",
    "ipatient = 0 # patient index\n",
    "x = Xs[ipatient,:].reshape(1, -1)\n",
    "x = pd.DataFrame.from_records(x, columns=names)\n",
    "ylabel = 'Case' if y[ipatient] else 'Control'\n",
    "print(\"{} Patient with (standardized) features:\\n{}\\n\".format(ylabel, x))\n",
    "\n",
    "# TODO: increase/decrease tobacco consumption\n",
    "x_copy = x.copy()\n",
    "x_copy['tobacco'] += 0 # offset to add in standardized unit\n",
    "\n",
    "# Proba of heart disease\n",
    "proba_CHD_0 = model.predict_proba(x)[0,1]\n",
    "proba_CHD = model.predict_proba(x_copy)[0,1]\n",
    "print(\"Proba of coronary heart disease (CHD): {:.3f}\".format(proba_CHD))\n",
    "print(\"Odds probability of CHD: {:.3f}\".format(proba_CHD/(1-proba_CHD)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice (*Optional*)\n",
    "-  Based on the logistic regression formula to compute the probability of each class  (with, or without CHD) and the values of the estimated weights, what would be the increase factor on the odds probability $$\\frac{ \\Pr(\\textrm{\"CHD\"} |X=x)}{\\Pr( \\textrm{\"no CHD\"}|X=x)}$$ when the tobacco consumption increases of 1 (in standardized unit)? \n",
    "- Check this by adding an offset  to the tobbaco variable in the cell above and comparing the obtained odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy variable selection procedure\n",
    "\n",
    "An alternative to Lasso penalization to select the most significant variables and regularize the problem is to fit all the possible combination of variables and choose the best one (for instance that minimizes the test error). This is called *best subset criterion*.  However this approach is very computationnaly demanding. With $p$ variables, we need to fit $2^p$ models. Additionally, if we want to use cross-validation to evaluate and compare their performances, the problem quickly becomes unfeasible...\n",
    "\n",
    "Greedy algorithm are then useful procedures that select the best variables (forward selection) or remove the worst variables (backward selection) **step by step**. There is generally no longer any guarantee of converging towards the best subset solution, but this may give useful solutions at a much reduced computational cost.\n",
    "\n",
    "### Exercise:\n",
    "- for our heart diseases dataset, usig a $K=5$-fold CV, how many fits would be required to find the best subset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward selection: Recursive Feature Elimination (RFE)\n",
    "\n",
    "Given a prediction rule that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select variables (the features) by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set.\n",
    "\n",
    "Within scikit-learn, recursive feature elimination is available with [`feature_selection.RFE`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html): for a linear model such that Logistic Regression, the importance of the features is obtained through their coefficients (pay attention to scale your data!). The recursive feature elimination is repeated until the desired number of features to select is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE, RFECV\n",
    "clf = LogisticRegression(penalty='none', tol=1e-6, max_iter=int(1e6))\n",
    "start = time()\n",
    "selector = RFE(clf, n_features_to_select=4, step=1)\n",
    "selector = selector.fit(Xs, y)\n",
    "print(\"This took %0.3fs\" % (time() - start))\n",
    "print(\"The selected variables are: {}\".format(np.array(names)[selector.support_]))\n",
    "sel_ind_sorted = np.argsort(selector.ranking_)\n",
    "ranking = pd.DataFrame.from_records(selector.ranking_[sel_ind_sorted].reshape(1,p), \n",
    "                                    columns=[names[i] for i in sel_ind_sorted], index=['Rank'])\n",
    "ranking.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "- Change the desired number of features to 1 to rank the features by signicance order (most significant variables must enter first in the model). Is it perfectly consistent with the Lasso path ranking?\n",
    "- What is hyperparameter to optimize for the RFE procedure?  How can you estimate it?\n",
    "- Replace the RFE procedure by [`feature_selection.RFECV`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html) to select the most significant variables. Is it in agreement with the Lasso results?\n",
    "- Do you think that this procedure is still appropriate when the number of variables $p$ is greater than the sample size $n$ of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward selection: Orthogonal Matching Pursuit (OMP)\n",
    "\n",
    "Orthogonal Matching Pursuit (OMP) is based on a greedy algorithm that selects at each step the most significant variable to enter the model. For a linear regression model, this is the variable\n",
    "most highly correlated with the current residual. It is similar to the simpler matching pursuit (MP) method, but better in that at each iteration, the residual is recomputed using an orthogonal projection on the space of the previously chosen variables.\n",
    "OMP was introduced in \n",
    "> G. Mallat, Z. Zhang, [Matching pursuits with time-frequency dictionaries](http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf), IEEE Transactions on Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
    "\n",
    "Note also that there exists an other popular method in the statistical literature called *Forward stepwise selection*. The selection step used differs from the one used in OMP in that it selects the variable that will lead to the minimum residual error *after* orthogonalisation. See for instance the following paper for a comparison between both methods\n",
    "> Blumensath, Thomas, and Mike E. Davies. [\"On the difference between orthogonal matching pursuit and orthogonal least squares.\"](https://eprints.soton.ac.uk/142469/1/BDOMPvsOLS07.pdf) (2007).\n",
    "\n",
    "This principle can be extented to generalized linear model such that Logistic Regression by replacing the residual sum of squares criterion by the opposite of the log likelihood. \n",
    "\n",
    "Within scikit-learn, only OMP is available for linear regression model with [`linear_model.OrthogonalMatchingPursuit`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html). However this can be directly apply to our binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Forward selection, aka Orthogonal Matching Pursuit\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV\n",
    "\n",
    "# Get the 5 most significant features\n",
    "omp = OrthogonalMatchingPursuit(n_nonzero_coefs=5,fit_intercept=True, normalize=True)\n",
    "omp.fit(X, y)\n",
    "coef = omp.coef_\n",
    "idx_r, = coef.nonzero()\n",
    "\n",
    "sel_feat_name= [names[l] for l in idx_r] ;\n",
    "print(\"Most significant features: {}\".format(np.array(names)[omp.coef_.nonzero()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "- Change the desired number of variables (number of nonzero coefs) from p to 1 to rank the features by signicance order (most significant variables must enter first in the model). Is it perfectly consistent with the previous results?\n",
    "- What is hyperparameter to optimize for the OMP procedure?  How can you estimate it?\n",
    "- Replace the OMP procedure by [`linear_model.OrthogonalMatchingPursuitCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html#sklearn.linear_model.OrthogonalMatchingPursuit) to select the most significant variables. Is it in agreement with the previous results?\n",
    "- *Optional:* Compared to greedy methods that are suboptimal procedures to approximate the best sparse solution (the best subset), Lasso penalty is often presented as a *convex relexation of the sparsity constraint*. Can you explain why? What are the possible benefits/disadvantages of the greedy and lasso procedures?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
