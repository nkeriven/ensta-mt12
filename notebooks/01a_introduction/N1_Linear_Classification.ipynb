{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/01a_introduction/N1_Linear_Classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random seed\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use scikit-learn to generate a toy 2D data set (two features $x_1$ and $x_2$)  for binary classification  (two classes) \n",
    " - each sample $(x_1,x_2)$ in the dataset is plotted as a 2D point where the two features $x_1$ and $x_2$ are displayed along the abscissa and ordinate axes respectively\n",
    " - the corresponding class label $y$ is displayed as a color mark (e.g., yellow or purple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "#X are the features (aka inputs, ...), y the labels (aka responses, targets, output...)\n",
    "X,y = make_classification(n_features=2, n_redundant=0, n_informative=2, n_samples=150,\n",
    "                          random_state=random_state, n_clusters_per_class=1)\n",
    "# make the class labels y_i as +1 or -1 (instead of 0)\n",
    "y[y==0]=-1\n",
    "# display the dataset\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.grid(True)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "#plt.savefig(\"2d_binary_classif.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a linear model is used to learn the classification function/rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# Train a linear model, namely  RidgeClassifier, \n",
    "# this includes standard linear regression as particular case (alpha=0)\n",
    "model = linear_model.RidgeClassifier(alpha=0)\n",
    "model.fit(X,y) # this is the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision functions\n",
    "XX, YY = np.meshgrid(np.linspace(X[:,0].min(), X[:,0].max(),200),\n",
    "                     np.linspace(X[:,1].min(), X[:,1].max(),200))\n",
    "XY = np.vstack([XX.flatten(), YY.flatten()]).T # create a grid of point. Shape (40000, 2)\n",
    "yp = model.predict(XY)\n",
    "plt.contour(XX,YY,yp.reshape(XX.shape)) # the contour function draw level sets\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.grid(\"on\")\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the parameter values of the linear boundary equation x_2=a x_1 + b?\n",
    "# coef and intercept are defined such that c^T x + b = 0\n",
    "a = -model.coef_[0][0]/model.coef_[0][1]\n",
    "b = -model.intercept_[0]/model.coef_[0][1]\n",
    "print('boundary equation x_2={} x_1 + {}'.format(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exo\n",
    "Draw directly the line in matplotlib with the function plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXO: draw a line y=ax+b\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.grid(\"on\")\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Change the number of informative features from  `n_informative=2̀` to `n_informative=1` in the `make_classification()` procedure, regenerate the data set and fit the classification rule. Interpret now the new decision boundary: are the two variables of equal importance in predicting the class of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the documentation for sklearn RidgeClassification object\n",
    "help(linear_model.RidgeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(make_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
