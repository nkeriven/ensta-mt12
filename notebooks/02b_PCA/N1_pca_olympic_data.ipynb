{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/02_PCA/N1_pca_olympic_data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Olympic decathlon data\n",
    "\n",
    "This example is a short introduction to PCA analysis. The Data are performance marks on the ten [decathlon events](https://en.wikipedia.org/wiki/Decathlon) for 33 athletes at the Olympic Games (1988)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below defines some useful functions to display summary statistics of PCA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "def scree_plot(pca):\n",
    "    \"\"\"bar plot of decreasing explained variance\n",
    "    \"\"\"\n",
    "    PC_values = np.arange(pca.n_components_) + 1\n",
    "    PC_labels = ['PC' + str(nb+1) for nb in range(pca.n_components_)] \n",
    "    plt.figure(figsize=(8,6))\n",
    "    # unlike in the course, in scikit learn, the explained_variance_ is directly the eigenvalues.\n",
    "    # The normalized \"explained variance\" in the course is the cumulative sum of explained_variance_ratio_\n",
    "    plt.bar(PC_values, pca.explained_variance_, linewidth=2, edgecolor='k')\n",
    "    plt.xticks(ticks=PC_values, labels=PC_labels)\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('Principal Components')\n",
    "    plt.ylabel('Eigenvalues')\n",
    "    plt.show()\n",
    "        \n",
    "def pca_summary(pca, X, out=True):\n",
    "    \"\"\"Display a table of the explained std, proportion of variance, \n",
    "    and proportion of variance ratio for each component\n",
    "    \"\"\"\n",
    "    names = [\"PC\"+str(i) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n",
    "    a = np.std(pca.transform(X), axis=0, ddof=1)\n",
    "    b = pca.explained_variance_ratio_\n",
    "    c = np.cumsum(pca.explained_variance_ratio_)\n",
    "    columns = pd.MultiIndex.from_tuples([(\"sdev\", \"Standard deviation\"), (\"varprop\", \"Proportion of Variance\"),\n",
    "                                         (\"cumprop\", \"Cumulative Proportion\")])\n",
    "    summary = pd.DataFrame(list(zip(a, b, c)), index=names, columns=columns)\n",
    "    if out:\n",
    "        print(\"Importance of components:\")\n",
    "        display(summary)\n",
    "    return summary\n",
    "\n",
    "def biplot2D(score,coeff,labels=None):\n",
    "    \"\"\"Generate biplot for the first two principal components \n",
    "    to display both scores and variables\n",
    "    \"\"\"\n",
    "    \n",
    "    xs = score[:,0] # projection on PC1\n",
    "    ys = score[:,1] # projection on PC2\n",
    "    p = coeff.shape[1]\n",
    "    n = score.shape[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    # plot the scores with the index of the sample\n",
    "    ax.scatter(xs, ys, marker=\".\", color = 'k')\n",
    "    for i in range(33):\n",
    "        ax.text(xs[i], ys[i], str(i), color = 'k')\n",
    "    ax.set_xlabel(\"PC{}\".format(1))\n",
    "    ax.set_ylabel(\"PC{}\".format(2))\n",
    "    \n",
    "    # plot the variable vectors (arrow) in the PC plane (loadings)  \n",
    "    arrow_sc = 1.15 \n",
    "    color = 'tab:red'\n",
    "    ax2 = ax.twinx() # instantiate a second x axe\n",
    "    ax2.set_ylim(-1.2,1.2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2 = ax2.twiny() # instantiate a second y axe\n",
    "    ax2.set_xlim(-1.2,1.2)\n",
    "    ax2.tick_params(axis='x', labelcolor=color)\n",
    "    for i in range(p):\n",
    "        ax2.arrow(0, 0, coeff[0, i], coeff[1, i], color =  color ,alpha = 0.5, \n",
    "                  linestyle = '-',linewidth = 1.5, head_width=0.02, head_length=0.02)\n",
    "        if labels is None:\n",
    "            ax2.text(coeff[0, i]* arrow_sc, coeff[1,i] * arrow_sc, \"Var\"+str(i+1), \n",
    "                     color = color, ha = 'center', va = 'center')\n",
    "        else:\n",
    "            ax2.text(coeff[0, i]* arrow_sc, coeff[1, i] * arrow_sc, labels[i], \n",
    "                     color = color, ha = 'center', va = 'center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Load olympic dataset contained in the text file `olympic.csv` (local user: you can also copy the file in the same directory as the notebook and directly load `olympic.csv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **Pandas**, a multi-purpose library for handling datasets in Python. Pandas has *many* functionality of which we will only use a fraction, see more at https://pandas.pydata.org/docs/getting_started/intro_tutorials/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#load data set\n",
    "olympic = pd.read_csv('https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/olympic.csv',\n",
    "                      sep=',', header=0)\n",
    "olympic.head() #data overview: variable names and first rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display some descriptive statistics for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olympic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can guess on the table above that the *running* event performances are measured in seconds, while the *jumping* or *throwing* ones are in meters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make *PCA* on decathlon event scores data $X \\in \\mathbb{R}^{n \\times p}$: $n=33$ samples (athletes), $p=10$ variables/features (decathlon events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "olympic_pc = pca.fit_transform(olympic) # get the Principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the distribution of component variances/eigenvalues $\\lambda_i^2$, $1 \\le i \\le p$ ? Let's visualize the **screeplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scree_plot(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a summary of PCA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_summary(pca, olympic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dsiplay the biplot\n",
    "\n",
    "The *biplot* gives a graphical summary of both samples (athletes) in terms of scores and the\n",
    "variables/features in terms of loadings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function. Use only the 2 PCs.\n",
    "biplot2D(olympic_pc[:,0:2], pca.components_[0:2, :], olympic.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot above, we see that the first principal component is positively associated with longer times on the 1500.  \n",
    "\n",
    "We can compare the athlete '1500' event mark with their score with the first component to check that slower runners will have higher value on this component, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average 1500 event mark (seconds) == {:.2f}'.format(olympic['1500'].mean()) )\n",
    "pd.DataFrame(list(zip(olympic['1500'],olympic_pc[:,0])), columns=['1500', 'score PC1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(olympic['1500'],olympic_pc[:,0],'k.')\n",
    "for i in range(len(olympic_pc[:,0])):\n",
    "    plt.text(olympic['1500'][i], olympic_pc[i,0], str(i), color = 'k')\n",
    "plt.xlabel('1500 time (s)')\n",
    "plt.ylabel('PC1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the correlation is almost perfect between the `1500`event and the first principal component!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the previous biplot shows that the *second main component* is correlated with the force in the form of a long *javelin* throw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(olympic['jave'],olympic_pc[:,1],'k.')\n",
    "for i in range(len(olympic_pc[:,0])):\n",
    "    plt.text(olympic['jave'][i], olympic_pc[i,1], str(i), color = 'k')\n",
    "plt.xlabel('Javelin throw (m)')\n",
    "plt.ylabel('PC2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check in the plot above that stronger throwers will have higher value on this second component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing: scale matters!\n",
    "\n",
    "In the previous example, we saw that the two variables were based somewhat on speed and strength. However, \n",
    "**we did not scale the variables** so the 1500 has much more weight than the 400, for instance! \n",
    "\n",
    "We correct this by standardizing the variables with `sklearn` preprocessor methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Center and reduce the variables\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(olympic)\n",
    "\n",
    "# Make PCA on standardized variables\n",
    "pca_s = PCA() # estimate only 2 PCs\n",
    "Xs_pc = pca_s.fit_transform(Xs) # project the original data into the PCA space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the new biplot for the standardized variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function. Use only the 2 PCs.\n",
    "biplot2D(Xs_pc[:,0:2], pca_s.components_[0:2, :], olympic.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By standardizing, this plot above reinforces our earlier interpretation by grouping sprint events (as *100m*,\n",
    "*110m*, *400m*, *long*) along a same axis aligned with the first principal  \n",
    "\n",
    "Likewise the strength and throwing events (in french, *javelot*, *disque*, *poids*)lies on a separate axis rather aligned on the second component (thus rather decorrelated from the previous one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_s.components_[0:2, :]\n",
    "pd.DataFrame(pca_s.components_[0:2, :].T, columns=['PC1', 'PC2'], index=olympic.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "- For the *non-standardized* olympic data, explain why the `1500` event is the more important to explain the variance. Is is still true after standardization?\n",
    "- Explain how many components do you think are sufficient to explain the *non-standardized* olympic data? Do you think the same is true for tje standardized data?\n",
    "- From the biplot analysis in the *standardized* case what are the global meanings of the first two principal components?  Are the loadings consistent with these conclusions?\n",
    "- In your opinion, is it better (i.e. more useful) to perform PCA on *standardized* or *non-standardized* data for this example?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
