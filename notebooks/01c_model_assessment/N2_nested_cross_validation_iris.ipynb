{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/01c_model_assessment/N2_nested_cross_validation_iris.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Nested versus non-nested cross-validation\n",
    "\n",
    "## Proper estimation of the test error\n",
    "\n",
    "This example compares non-nested and nested cross-validation strategies on a\n",
    "classifier of the iris data set. Nested cross-validation (CV) is often used to\n",
    "train a model in which hyperparameters also need to be optimized. Nested CV\n",
    "estimates the generalization error of the underlying model and its\n",
    "(hyper)parameter search. Choosing the parameters that maximize non-nested CV\n",
    "biases the model to the dataset, yielding an overly-optimistic score.\n",
    "\n",
    "Model selection without nested CV uses the same data to tune model parameters\n",
    "and evaluate model performance. Information may thus \"leak\" into the model\n",
    "and overfit the data. The magnitude of this effect is primarily dependent on\n",
    "the size of the dataset and the stability of the model. See Cawley and Talbot\n",
    "[1] for an analysis of these issues.\n",
    "\n",
    "To avoid this problem, nested CV effectively uses a series of\n",
    "train/validation/test set splits. In the inner loop (here executed by\n",
    ":class:`GridSearchCV <sklearn.model_selection.GridSearchCV>`), the score is\n",
    "approximately maximized by fitting a model to each training set, and then\n",
    "directly maximized in selecting (hyper)parameters over the validation set. In\n",
    "the outer loop (here in :func:`cross_val_score\n",
    "<sklearn.model_selection.cross_val_score>`), generalization error is estimated\n",
    "by averaging test set scores over several dataset splits.\n",
    "\n",
    "The example below uses a [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "with optimized hyperparameters by grid search. We compare the\n",
    "performance of non-nested and nested CV strategies by taking the difference\n",
    "between their scores.\n",
    "\n",
    "\n",
    "     [1] `Cawley, G.C.; Talbot, N.L.C. On over-fitting in model selection and\n",
    "     subsequent selection bias in performance evaluation.\n",
    "     J. Mach. Learn. Res 2010,11, 2079-2107.\n",
    "     <http://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf>`_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 30\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Set up possible values of parameters to optimize over\n",
    "p_grid = {\"n_neighbors\": [1,4,7,10],\n",
    "          \"metric\": [\"euclidean\", \"manhattan\", \"chebyshev\"]}\n",
    "\n",
    "\n",
    "# We will use a nearest neighbors classifier\n",
    "knn = KNeighborsClassifier() # Creation of the classifier\n",
    "\n",
    "# Arrays to store scores\n",
    "non_nested_scores = np.zeros(NUM_TRIALS)\n",
    "nested_scores = np.zeros(NUM_TRIALS)\n",
    "\n",
    "# Loop for each trial\n",
    "for i in range(NUM_TRIALS):\n",
    "\n",
    "    # Choose cross-validation techniques for the inner and outer loops,\n",
    "    # independently of the dataset.\n",
    "    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n",
    "    inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "    outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "    \n",
    "    # Non_nested hyperparameter search and scoring\n",
    "    clf = GridSearchCV(estimator=knn, param_grid=p_grid, cv=inner_cv)\n",
    "    clf.fit(X_iris, y_iris)\n",
    "    non_nested_scores[i] = clf.best_score_\n",
    "    #print(clf.best_estimator_)\n",
    "    \n",
    "    # Nested CV with parameter optimization to get a more accurate estimation \n",
    "    # of the test error with the hyperparameters selected in the previous (inner) CV. \n",
    "    nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv)\n",
    "    nested_scores[i] = nested_score.mean()\n",
    "\n",
    "score_difference = non_nested_scores - nested_scores\n",
    "\n",
    "print(\"Average difference of {:6f} with std. dev. of {:6f}.\"\n",
    "      .format(score_difference.mean(), score_difference.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores on each trial for nested and non-nested CV\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "non_nested_scores_line, = plt.plot(non_nested_scores, color='r')\n",
    "nested_line, = plt.plot(nested_scores, color='b')\n",
    "plt.ylabel(\"score\", fontsize=\"14\")\n",
    "plt.legend([non_nested_scores_line, nested_line],\n",
    "           [\"Non-Nested CV\", \"Nested CV\"],\n",
    "           bbox_to_anchor=(0, .4, .5, 0))\n",
    "plt.title(\"Non-Nested and Nested Cross Validation on Iris Dataset\",\n",
    "          x=.5, y=1.1, fontsize=\"15\")\n",
    "\n",
    "# Plot bar chart of the difference.\n",
    "plt.subplot(212)\n",
    "difference_plot = plt.bar(range(NUM_TRIALS), score_difference)\n",
    "plt.xlabel(\"Individual Trial #\")\n",
    "plt.legend([difference_plot],\n",
    "           [\"Non-Nested CV - Nested CV Score\"],\n",
    "           bbox_to_anchor=(0, 1, .8, 0))\n",
    "plt.ylabel(\"score difference\", fontsize=\"14\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "  1. Explain why the prediction performances estimated by non-nested CV \n",
    "    is optimistic with respect to the nested CV ones.\n",
    "  2. Which estimator do you think most reliable for the test error?\n",
    "\n",
    "A current alternative in machine learning is to split the original dataset to get a **separate test set**.\n",
    "A standard (i.e. non nested) cross-validation procedure can then be applied on the remaining data set. The samples in the test set are now only used to evaluate the test error.\n",
    "This is different to nested cross-validation where the test samples used in the outer-loop are also used in the inner-loop to train or validate the model.\n",
    "\n",
    "  3. Why use a separate test set in addition to the validation set?\n",
    "  4. What are the benefits (or disavantages) of nested cross-validation w.r.t. the \"separate test set\" split approach?\n",
    "  5. **Optional:** Implement the \"separate test set\" approach on this example and compare with the nested cross-validation (*hint:* you may use `train_test_split()`, `GridSearchCV()`, `cross_val_score()` sklearn functions). Compare and interpret your results.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "1. The risk is to get too specific hyperparameters that overfit the validation sets. In this case, the estimated test error fot these hyperparameters may be too optimistic.\n",
    "2. Nested cross validation makes use of an outer-loop to evaluate the performances on some validation sets called *test sets* (as they are only used to evaluate the test error, not to select a model), that differ from the ones used in the inner-loop. Thus the risk that the hyperparameters selected in the CV inner-loop also overfit these test sets is much reduced. This yields a more reliable estimate. We can check this here on the simulation results: the non nested CV is almost always slightly too optimistic because we tend to overfit the inner-loop validation sets when we select the best hyperparameters.\n",
    "3. Using a separate test set, which is not used fot training or hyperparameter selection, yields an unbiased estimate of the true error (the test error)\n",
    "4. Both methods give proper or accurate estimation of the test error. \n",
    "\n",
    " Advantages of the \"separate test set\" split approach:\n",
    " \n",
    "  - conceptually easier to understand\n",
    "  - nested cross validation is more computationnaly intensive as it requires an additional CV in the outer-loop.\n",
    " \n",
    " Main advantage of the \"nested CV\":\n",
    " \n",
    "  - Nested CV does not require to split the original dataset. All the available samples can be used in the two CV loops conversely to the \"separate test set\" approach. This makes possible to obtain more performant prediction rules with a proper estimation of the associated test error.\n",
    "5. To do by yourself\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
