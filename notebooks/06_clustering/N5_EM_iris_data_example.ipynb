{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/06_clustering/N5_EM_iris_data_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# GMM covariances\n",
    "\n",
    "\n",
    "Demonstration of several covariances types for Gaussian mixture models.\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture for more information on the estimator.\n",
    "\n",
    "Although GMM are often used for clustering, we can compare the obtained\n",
    "clusters with the actual classes from the dataset. We initialize the means\n",
    "of the Gaussians with the means of the classes from the training set to make\n",
    "this comparison valid.\n",
    "\n",
    "We plot predicted labels on both training and held out test data using a\n",
    "variety of GMM covariance types on the iris dataset.\n",
    "We compare GMMs with spherical, diagonal, full, and tied covariance\n",
    "matrices in increasing order of performance. Although one would\n",
    "expect full covariance to perform best in general, it is prone to\n",
    "overfitting on small datasets and does not generalize well to held out\n",
    "test data.\n",
    "\n",
    "On the plots, train data is shown as dots, while test data is shown as\n",
    "crosses. The iris dataset is four-dimensional. Only the first two\n",
    "dimensions are shown here, and thus some points are separated in other\n",
    "dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simplified version of the code that can be found here:\n",
    "https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris data - EM clustering\n",
    "Note that while labels are available for this data set, they will not be used in GMM identification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X_train = iris.data\n",
    "y_train = iris.target\n",
    "\n",
    "n_classes = len(np.unique(y_train))  #list the unique elements in y_train : nb of different labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM model estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try GMMs using full covariance (no constraints imposed on cov)\n",
    "estimator = GaussianMixture(n_components=n_classes, \n",
    "                             covariance_type='full', max_iter=50, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !! \n",
    "Lines below initialize with centers of mass of each clsuetr, as labels are known...\n",
    "Usually, 3 different centers are required, chosen at random. It this latter case, the correct\n",
    "    clusters are extracted, upto some circular permutation on the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have class labels for the training data, we can\n",
    "# initialize the GMM parameters in a supervised manner.\n",
    "estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)\n",
    "                                    for i in range(n_classes)])\n",
    "\n",
    "estimator.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimator.covariances_)\n",
    "#print(estimator.covariances_[1][1:3,1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting results : \n",
    "choose the  axis pair to visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for K clusters, specify K colors  (here K=3)\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={'aspect': 'equal'})\n",
    "\n",
    "axes=[1,3] # between 0 and 3\n",
    "for k in range(n_classes):\n",
    "        # defines ellipses parameters, using eigen-axes\n",
    "        data = iris.data[iris.target == k]\n",
    "        covariances = estimator.covariances_[k][np.ix_(axes,axes)]\n",
    "        plt.scatter(data[:, axes[0]], data[:, axes[1]], s=10, color=colors[k],\n",
    "                    label=iris.target_names[k])\n",
    "        Est_means=estimator.means_[k,axes]\n",
    "        \n",
    "        v, w = np.linalg.eigh(covariances) \n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi  # convert to degrees\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        ell = mpl.patches.Ellipse(Est_means, v[0], v[1],\n",
    "                                  180 + angle, color=colors[k])\n",
    "        #dplot the ellipses\n",
    "        ell.set_clip_box(ax.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "        ax.set_aspect('auto')\n",
    "     \n",
    "        \n",
    "# for visualizing axe1 vs axe2, use \"covariances = estimator.covariances_[n][0:2, 0:2]\"\n",
    "# for visualizing axe1 vs axe3, use \"covariances = estimator.covariances_[n][0::2, 0::2]\"\n",
    "# for visualizing axe1 vs axe4, use \"covariances = estimator.covariances_[n][0::3, 0::3]\"\n",
    "# for visualizing axe2 vs axe3, use \"covariances = estimator.covariances_[n][1:3, 1:3]\"\n",
    "# for visualizing axe2 vs axe4, use \"covariances = estimator.covariances_[n][1::2, 1::2]\"\n",
    "# for visualizing axe3 vs axe4, use \"covariances = estimator.covariances_[n][2:, 2:]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from scipy import linalg\n",
    "from sklearn import mixture\n",
    "\n",
    "lowest_bic = np.infty\n",
    "bic = []\n",
    "aic = []\n",
    "n_components_range = range(2, 7)\n",
    "cv_type =  'full'\n",
    "\n",
    "    \n",
    "for n_comp in n_components_range:\n",
    "        # Fit a Gaussian mixture with EM\n",
    "    gmm = GaussianMixture(n_components=n_comp,\n",
    "                          covariance_type=cv_type, max_iter=1000, random_state=1)\n",
    "    gmm.fit(X_train)\n",
    "    #bic.append(gmm.aic(X_train))\n",
    "    bic.append(gmm.bic(X_train))\n",
    "    aic.append(gmm.aic(X_train))\n",
    "bic = np.array(bic)\n",
    "aic = np.array(aic)\n",
    "\n",
    "# Plot the BIC scores\n",
    "\n",
    "plt.plot(np.linspace(2,6,5),bic,'b',np.linspace(2,6,5),aic,'r')\n",
    "\n",
    "print(\"bic = {}\".format(bic))\n",
    "print(\"aic = {}\".format(aic))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercize\n",
    "Play with the covariance type: see https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is time...\n",
    "What about PCA ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
