{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/05_SVM/N1_plot_svm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM: Maximum margin separating hyperplane\n",
    "\n",
    "Plot the maximum margin separating hyperplane within a two-class\n",
    "separable or not dataset using a Support Vector Machine classifier with\n",
    "linear and nonlinear kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def svm_fit_disp(clf, separable=True):\n",
    "    \"\"\"\n",
    "    1. make a simple two-class dataset of 2D points \n",
    "       to fit a Support Vector Machine classifier 'clf'.\n",
    "       By defaut the dataset is separable.\n",
    "     \n",
    "    2. display the  maximum margin separating hyperplane and \n",
    "       the margin boundaries and the support vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # we create 40 separable, or not,  2D points\n",
    "    X, y = make_classification(n_samples=40, n_features=2, n_informative=2,\n",
    "                               n_redundant=0, class_sep= 2, random_state=2,\n",
    "                               flip_y=0 if separable else 0.1)\n",
    "    # fit the model\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # plot the training samples\n",
    "    fig, ax = plt.subplots(1,1,figsize=(9,6))\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "    # plot the decision function\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 60)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 60)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    # plot decision boundary and margins\n",
    "    cs = ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "    # Recast levels with decision boundary and margin labels\n",
    "    cs.levels = ['margin','hyperplane','margin']\n",
    "    ax.clabel(cs, cs.levels, inline=True, fontsize=12)\n",
    "    # plot support vectors\n",
    "    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k',label='support vectors')\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise I (linear kernel):**\n",
    "\n",
    "Remember that for _soft-margin_ SVM, weâ€™re trying to maximize the margin for the separating hyperplane, while accounting for a penalty when a sample is either misclassified or within the margin boundary. \n",
    "The penalty parameter $C$ controls the strengh of this penalty and trades off correct classification of training samples against maximization of the decision function's margin.\n",
    "- for larger values of $C$, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. \n",
    "- a lower $C$ will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy.\n",
    "\n",
    "In other words $C$ behaves as an inverse regularization parameter in the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_fit_disp(svm.SVC(kernel='linear', C=100), separable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Questions*\n",
    "1. Separable dataset \n",
    " - In this separable dataset does the SVM solution (here, a large regularization parameter  $C=100$) separates perfectly the two-class training samples? How many are the vector supports? \n",
    " - Why when $C$ is large the solution for a separable dataset  converges to the _hard-margin_ case? \n",
    " - Gradually decrease the regularization parameter  `C` s.t. $C=100, 10, 1, 0.1, 0.01$. Note the possible changes for both the decision function and the margins/vector supports. Does the \n",
    "decision boudary still separate perfectly the two classes?\n",
    "2. Nonseparable dataset\n",
    " - Set`separable=False`  to make the dataset nonseparable, and play on the values of C as above. Does the solution still strongly depend on $C$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise II (nonlinear kernel):**\n",
    "\n",
    "The gaussian, also known as radial basis function (RBF), kernel is defined as:\n",
    "$$\n",
    "K(x, y) = \\exp(-\\gamma ||x-y||^2),\n",
    "$$\n",
    "where the $\\gamma>0$ parameter defines how far the influence of a single training example reaches, with low values meaning 'far' and high values meaning 'close'. \n",
    "\n",
    "The behavior of the radial basis function (RBF) svm kernel is very sensitive to the $\\gamma$ parameter:\n",
    "- if $\\gamma$ is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with C will be able to prevent overfitting.\n",
    "\n",
    "- when $\\gamma$ is very small, the model is too constrained and cannot capture the complexity or \"shape\" of the data. The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_fit_disp(svm.SVC(kernel='rbf', gamma=0.01, C=1), separable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Questions*\n",
    "- For the current SVM parameters, can you explain why the solution is not very different from the linear kernel?\n",
    "- What do you observe for a larger value of $C$, e.g. `C=1000`? Does the separating hyperplane is still a true hyperplane in the input space?\n",
    "- increase gradually the RBF kernel parameter  $\\gamma$ from $0.01$ to $10$ (you can reset the regularization parameter to the previous value `C=1`). What do you observe? What should be appropriate values of $\\gamma$ here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
