{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/05_SVM/N2_svm_zip_digits.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten digits recognition\n",
    "\n",
    "This is a continuation of the notebook done previously during the lab on Discriminant Analysis.\n",
    " \n",
    "Remember that we aim to recognize handwritten digits in digital images.\n",
    "After normalization (see [here](http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.info.txt) for more details on these data, which are taken from zip codes on US postal envelopes), the resulting images are composed of $16 \\times 16$ pixels, each pixel being quantified in gray levels in the interval $[- 1.1]$.\n",
    "Thus, we  have $X \\in [-1,1]^{256}$, i.e $p = 256$, and $Y \\in \\{0,1, \\ldots, 9 \\}$.\n",
    "\n",
    "The code cells below allow to\n",
    "- [Display some images from each class](#Display-some-digits)\n",
    "- [Get the results for LDA](#Recap-on-Regularized-linear-discriminant-analysis-results)\n",
    "- [Focus on binary classification task for a couple of digits](#Binary-classification)\n",
    "- [Use Non-Linear kernels](#Non-Linear-SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load (full) data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/zip_train_full.mat -O zip_train_full.mat\n",
    "!wget https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/zip_test_full.mat -O zip_test_full.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "# Warning: put the data files in the notebook directory\n",
    "data = spio.loadmat(\"zip_train_full.mat\")\n",
    "Xtrain = data[\"Xtrain_full\"]\n",
    "Ytrain = data[\"Ytrain_full\"]\n",
    "Xshape = Xtrain.shape\n",
    "Ytrain = np.reshape(Ytrain, (Xshape[0],))\n",
    "Yshape = Ytrain.shape\n",
    "\n",
    "print(\"Xtrain is (n={},p={}) sized\".format(Xshape[0], Xshape[1]))\n",
    "print(\"Ytrain is a (n={},) sized vector of reponses\".format(Yshape[0]))\n",
    "\n",
    "data_test = spio.loadmat(\"zip_test_full.mat\")\n",
    "Xtest = data_test[\"Xtest_full\"]\n",
    "Ytest = data_test[\"Ytest_full\"]\n",
    "Ytest = np.reshape(Ytest, (Xtest.shape[0],))\n",
    "print(\"Xtest is (n={},p={}) sized\".format(Xtest.shape[0], Xtest.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))  # to specify the size of the images\n",
    "for i, ind in enumerate(np.random.choice(Xshape[0], 10)):\n",
    "    fig.add_subplot(2, 5, i+1)\n",
    "    mplot = plt.imshow(np.reshape(Xtrain[ind,:], (16, 16)), cmap=\"gray_r\")\n",
    "    plt.title(Ytrain[ind])\n",
    "    mplot.axes.get_xaxis().set_visible(False)\n",
    "    mplot.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap on Regularized *linear* discriminant analysis results\n",
    "\n",
    "Here are the multiclass class results for (regularized) linear discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Fit and test Regularized LDA\n",
    "#qda = QDA(reg_param=.1)\n",
    "lda = LDA(solver='lsqr', shrinkage='auto')\n",
    "lda.fit(Xtrain, Ytrain)\n",
    "\n",
    "y_hat = lda.predict(Xtest)\n",
    "mcr_test = np.mean(Ytest != y_hat)\n",
    "\n",
    "print(\"Regularized LDA test mcr: {:0.3f}\".\n",
    "          format(mcr_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the *confusion matrix*\n",
    "\n",
    "The *confusion matrix* (see the [scikitlearn user guide](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) for some examples) is a useful tool in supervised learning to evaluate classification accuracy. Quoting Wikipedia:\n",
    ">Each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class (or vice versa, depending on the convention). The name stems from the fact that it makes it easy to see the confusion between two classes (i.e. commonly mislabeling one as another). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Ytest, y_hat) # y_hat is the auto regularized LDA prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(lda, Xtest, Ytest)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "- For regularized LDA, what are the the most common confusions between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select in the following a couple of digits with the most confusions among predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a couple of digits with the most confusions among predictions\n",
    "digit_pair = [3, 5] \n",
    "sel_pair = np.logical_or(Ytrain == digit_pair[0], \n",
    "                         Ytrain == digit_pair[1])\n",
    "X_train = Xtrain[ sel_pair ]\n",
    "y_train = Ytrain[ sel_pair ]\n",
    "\n",
    "sel_pair_test = np.logical_or(Ytest == digit_pair[0], \n",
    "                              Ytest == digit_pair[1])\n",
    "X_test = Xtest[ sel_pair_test ]\n",
    "y_test = Ytest[ sel_pair_test ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refit a LDA model anc compute its accuracy for this two-class dataset (remember that due do the global covariance matrix assumption in LDA, results should not be the same for the multiclass problem or for a particular binary one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# train the LDA model\n",
    "start = time()\n",
    "lda = LDA(solver='lsqr', shrinkage='auto')\n",
    "lda.fit(X_train, y_train)\n",
    "print(\"This took %0.3fs\" % (time() - start))\n",
    "\n",
    "y_pred = lda.predict(X_test)\n",
    "mcr_test = np.mean(y_test != y_pred)\n",
    "\n",
    "print(\"Regularized LDA mcr on the test set:: {:0.3f}\".\n",
    "          format(mcr_test))\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "\n",
    "Within scikit-learn, the `LinearSVC` class is optimized for linear *only* SVM classifier.\n",
    "This is similar to `SVC` with parameter `kernel='linear'`, but should scale better to large numbers of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from time import time\n",
    "\n",
    "# train the linear SVM\n",
    "start = time()\n",
    "clf = LinearSVC(max_iter=10000, C=.01)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"This took %0.3fs\" % (time() - start))\n",
    "\n",
    "# Predict new samples\n",
    "y_pred = clf.predict(X_test)\n",
    "# Compute the overall accuracies\n",
    "mcr_test = np.mean(y_test != y_pred)\n",
    "print(\"Linear SVM mcr on the test set: {}\".format(mcr_test))\n",
    "print('confusion matrix:')\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "- Does linear SVM perform better (or signicantly better) than LDA?\n",
    "- What are the options for improving performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear SVM\n",
    "We now consider a *second order polynomial* kernel to use with the SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "degree = [2] # Degree of the polynomial kernel\n",
    "C = np.logspace(-1, 4, 6) # Penality of the optimization problem\n",
    "param_grid = [dict(kernel=['poly'], degree=degree, C=C)]\n",
    "\n",
    "# CV estimate for the kernel + regularization paramaters\n",
    "grid = GridSearchCV(SVC(),  # Set up the classifier\n",
    "                    param_grid=param_grid, \n",
    "                    cv= 5,\n",
    "                    n_jobs=-1) # Do the grid search in parallel\n",
    "start = time()\n",
    "grid.fit(X_train, y_train) # Run the grid search\n",
    "print(\"This took %0.3fs\" % (time() - start))\n",
    "\n",
    "print(\"Best score: {}\".format(grid.best_score_))\n",
    "print(\"Best set of hyperparameters: {}\".format(grid.best_params_))\n",
    "\n",
    "# Learn the optimal model\n",
    "clf = grid.best_estimator_  # Get the best estimator\n",
    "clf.fit(X_train,y_train)  # Fit it using the training set\n",
    "\n",
    "\n",
    "# Predict new samples\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the overall accuracies\n",
    "mcr_test = np.mean(y_test != y_pred)\n",
    "print(\"Polynomial SVM mcr on the test set: {}\".format(mcr_test))\n",
    "print('confusion matrix:')\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "- Does this non-linear kernel signicantly improve the prediction performance?\n",
    "- Should a quadratic discriminant analysis (QDA) also work well for this problem? (*Optional:*  check this, using a samll amount of regularization for QDA, e.g. `qda = QDA(reg_param=.1)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we would like to compare several kernels. We need can use the same workflow, just by adding few lines to define the range of the parameters!. \n",
    "\n",
    "Note that we are going here to optimize all the parameters using grid search, i.e., a  brute force strategy. It exists several algorithms to do it properly, e.g., https://arxiv.org/abs/1602.02355 thus reducing the computational burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = np.arange(1, 4) # Degree values for the polynomial kernel\n",
    "gamma = np.logspace(-3, 0, 9) # Scale values for the RBF kernel\n",
    "C = [1] # Penality values for the optimization criterion\n",
    "param_grid = [dict(kernel=['rbf'], gamma=gamma, C=C),\n",
    "              dict(kernel=['poly'], degree=degree, C=C)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now now copy/past/run the same code than previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(SVC(),  # Set up the classifier\n",
    "                    param_grid=param_grid, \n",
    "                    cv= 5,\n",
    "                    n_jobs=-1) # Do the grid search in parallel\n",
    "start = time()\n",
    "grid.fit(X_train, y_train) # Run the grid search\n",
    "print(\"This took %0.3fs\" % (time() - start))\n",
    "\n",
    "print(\"Best score: {}\".format(grid.best_score_))\n",
    "print(\"Best set of hyperparameters: {}\".format(grid.best_params_))\n",
    "\n",
    "# Learn the optimal model\n",
    "clf = grid.best_estimator_  # Get the best estimator\n",
    "clf.fit(X_train,y_train)  # Fit it using the training set\n",
    "\n",
    "\n",
    "# Predict new samples\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the overall accuracies\n",
    "mcr_test = np.mean(y_test != y_pred)\n",
    "print(\"Kernel SVM mcr on the test set: {}\".format(mcr_test))\n",
    "print('confusion matrix:')\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "- Which kernel gives the best results here?\n",
    "- Does this non-linear kernel significantly improve the prediction performance w.r.t. the linear case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the errors now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors\n",
    "fig = plt.figure(figsize=(12, 5))  # to specify the size of the images\n",
    "t = np.where(y_pred!=y_test)[0]\n",
    "for k, t_ in enumerate(t[:10]):\n",
    "    fig.add_subplot(2, 5, k+1)\n",
    "    mplot = plt.imshow(np.reshape(X_test[t_], (16, 16)), cmap=\"gray_r\")\n",
    "    plt.title(\"Label = {}\".format(y_test[t_]))\n",
    "    # hide the axis\n",
    "    mplot.axes.get_xaxis().set_visible(False)\n",
    "    mplot.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "- Do you think that there is still room for improvement? Can we be more flexible than using a standard polynomial or RBF kernel?\n",
    "- What about the computational time?\n",
    "- *Optional:* redo the same operation for another pair of digits. Check if your results are still consistents.\n",
    "- *Optional:* Run a kernel SVM for the original multiclass problem (using OVR strategy). Is the performance gain still as significant compared to LDA? Compared to QDA (using a small amount of regularization for QDA, e.g. `qda = QDA(reg_param=.1)`))?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
