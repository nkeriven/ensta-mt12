{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/05_SVM/N5_importance_of_scaling-svm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of scaling: linear and kernel SVMs\n",
    "Experiment the importance of scaling on a toy example for both linear and non-linear SVMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate (unscaled) data\n",
    "The first variable is the informative one, but is three magnitude orders smaller than the second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.random.seed(0)\n",
    "X = sp.random.randn(200,2)\n",
    "X[:,0] *= 0.1\n",
    "X[:,1] *= 100\n",
    "\n",
    "# Create a vertical class boundary: depends on the first variable X[:,0]  only\n",
    "y = np.ones((200,))\n",
    "y[X[:,0]<0]=0\n",
    "\n",
    "# Plot the dataset\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=cm_bright,\n",
    "    edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the second variable (ordinate) is **three magnitude orders larger** than the first one (abcissa)\n",
    " \n",
    "We train now a SVM classifier on these unscaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of SVM  (rbf kernel) classifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=0, stratify=y)\n",
    "model = SVC(kernel='rbf',gamma='auto')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification accuracy before scaling: {}\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display decision boundary\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, (x_max-x_min)/1000),\n",
    "                     np.arange(y_min, y_max,(y_max-y_min)/1000 ))\n",
    "# just plot the dataset first\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# Plot the training points\n",
    "fig, ax= plt.subplots(figsize=(10,6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "    edgecolors='k')\n",
    "plt.title('Unscaled', fontsize=14)\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "\n",
    "Z = model.decision_function(sp.c_[xx.ravel(), yy.ravel()])\n",
    "cs = ax.contour(xx, yy, Z.reshape(xx.shape), [0]) \n",
    "cs.levels = ['hyperplane']\n",
    "ax.clabel(cs, cs.levels, inline=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice:\n",
    "- Is this SVM classifier really useful (compare with the trivial uniform classifier)? \n",
    "- How can we interpret the shape of the decision boundary ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minmax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Scale data between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "X_train_s= sc.fit_transform(X_train) # Scale data between 0 and 1\n",
    "X_test_s= sc.transform(X_test) # Scale data between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train now a SVM classifier on these SCALED data\n",
    "\n",
    "model = SVC(kernel='rbf',gamma='auto')\n",
    "model.fit(X_train_s,y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "print(\"Classification accuracy after scaling: {}\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data (centered with unit variance)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_s= sc.fit_transform(X_train)\n",
    "X_test_s= sc.transform(X_test)\n",
    "\n",
    "model = SVC(kernel='rbf',gamma='auto')\n",
    "model.fit(X_train_s, y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "print(\"Classification accuracy after standardization: {}\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display decision boundary\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "x_min, x_max = X_train_s[:, 0].min() - .5, X_train_s[:, 0].max() + .5\n",
    "y_min, y_max = X_train_s[:, 1].min() - .5, X_train_s[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, (x_max-x_min)/100),\n",
    "                     np.arange(y_min, y_max,(y_max-y_min)/100 ))\n",
    "# just plot the dataset first\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# Plot the training points\n",
    "fig, ax= plt.subplots(figsize=(10,6))\n",
    "plt.scatter(X_train_s[:, 0], X_train_s[:, 1], c=y_train, cmap=cm_bright,\n",
    "    edgecolors='k')\n",
    "plt.title('Scaled', fontsize=14)\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "\n",
    "Z = model.decision_function(sp.c_[xx.ravel(), yy.ravel()])\n",
    "cs = ax.contour(xx, yy, Z.reshape(xx.shape), [-1, 0,1 ])\n",
    "cs.levels = ['margin','hyperplane','margin']\n",
    "ax.clabel(cs, cs.levels, inline=True, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice   \n",
    "- Explain why SVM is sensitive to the scaling of the data\n",
    "- Does the RBF kernel allow the correct separation of the unscaled data? What about the scaled data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a different kernel, for instance a linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit unscaled data\n",
    "model = SVC(kernel='linear', C=.1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification accuracy **before scaling** with linear kernel: {}\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaled data\n",
    "model = SVC(kernel='linear', C=.1)\n",
    "model.fit(X_train_s,y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "print(\"Classification accuracy **after scaling** with linear kernel: {}\".format(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice\n",
    "- Is the linear kernel appropriate to separate the two class (*hint:* what is the true optimal separating function for this toy dataset)?\n",
    "- What happen for the linear kernel for larger values of $C$, e.g. `C=1`, `C=10`, `C=100`or `C=1000`? Can you explain why in this case the accuracy is improved by increasing the cost parameter $C$ even for **unscaled data**?\n",
    "- Add some noise (e.g. change the label for some data points) add check if it still works (good accuracy for unscaled data with large C)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
