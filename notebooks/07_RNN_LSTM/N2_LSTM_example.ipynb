{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/07_RNN_LSTM/N2_LSTM_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM example on forecasting a scalar temperature value from a vector of past values. \n",
    "\n",
    "This Notebook is largely inspired from \n",
    "https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "which runs under TensorFlow2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# if tensorflow2, replace by\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# to insure reproductability of different runs\n",
    "np.random.seed(1)\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(2)\n",
    "\n",
    "# if tensorflow2, replace the two lines above by\n",
    "from tensorflow import random\n",
    "\n",
    "random.set_seed(2)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses a weather time series dataset recorded by the Max Planck Institute for Biogeochemistry.\n",
    "\n",
    "This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by François Chollet for his book Deep Learning with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zip_path = keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip', extract= True)\n",
    "csv_path, _ = os.path.splitext(zip_path)\n",
    "# csv_path\n",
    "df = pd.read_csv(csv_path)\n",
    "#filename = \"jena_climate_2009_2016.csv\"\n",
    "#df = pd.read_csv(filename) #This is a pandas DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An observation is recorded every 10 minutes. This means that, for a single hour, you will have 6 observations. Similarly, a single day will contain 144 (6x24) observations.\n",
    "\n",
    "Given a specific time, let's say you want to predict the temperature 6 hours in the future. In order to make this prediction, you choose to use a set of preceding N observations. Thus, you would create a window containing the last N points. For example, 5 days of observation correspond to 720(5x144) observations to train the model. Many such configurations are possible, making this dataset a good one to experiment with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Forecast a univariate time series\n",
    "\n",
    "Here, you will train a model using only a single feature (temperature), and use it to make predictions for that value in the future.\n",
    "\n",
    "Let's first extract only the temperature from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract univariate time series\n",
    "Temp = df[\"T (degC)\"]\n",
    "Temp.index = df[\"Date Time\"]\n",
    "print(f\"the time series contains {Temp.shape[0]} sample\")\n",
    "print(\"\\n First five rows:\")\n",
    "Temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot time series\n",
    "Temp.plot(subplots=True, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling\n",
    "\n",
    "LSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. It can be a good practice to rescale the data to the range of 0-to-1 (if sigmoid activation) or -1 to +1 (if tanh), also called normalizing. We can easily normalize the dataset using the MinMaxScaler preprocessing class from the scikit-learn library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(Temp.values, dtype='float32')[:,np.newaxis]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = np.array(Temp.values, dtype='float32')[:,np.newaxis]\n",
    "\n",
    "#data = np.reshape(Temp.values, (420551, 1))\n",
    "#data = data.astype(\"float32\")\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))  # compatiblity with choice activation=tanh\n",
    "data_sc = scaler.fit_transform(data)\n",
    "\n",
    "print(data[:5,])\n",
    "scaler.inverse_transform(data_sc)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Implement the minmaxscaler yourself, verify your result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train_size = 300000\n",
    "test_size = len(data_sc) - train_size\n",
    "train, test = data_sc[0:train_size, :], data_sc[train_size : len(data_sc), :]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "why don't we use the train/test splitting function from sklearn ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange data into a matrix forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a dataset of $p$-dimensional samples and labels. Each time step $i$ will correspond to a sample: the previous time steps $[x_{i-p+1}, \\ldots, x_{i-1}, x_i]$ are used as the sample, from which we want to predict some future value $x_{i+\\ell}$.\n",
    "\n",
    "The following function create such a data matrix. $p$ is called look_back, and $\\ell$ is called forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1, forecast=0):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(look_back, len(dataset) - forecast):  # current time index is i\n",
    "        a = dataset[i-look_back : i, 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(\n",
    "            dataset[i + forecast, 0]\n",
    "        )  # from a, predicts at time index i\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the example below, only 20 past observations are used to predict future  temperature value 6 samples ahead (This correspond to a 1 hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_back = 20\n",
    "forecast = 6\n",
    "trainX, trainY = create_dataset(train, look_back, forecast)\n",
    "testX, testY = create_dataset(test, look_back, forecast)\n",
    "trainX.shape\n",
    "trainY = trainY.reshape((-1, 1))\n",
    "testY = testY.reshape((-1, 1))\n",
    "print(f'Number of training samples:{trainX.shape[0]}, number of test samples:{testX.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LSTM network expects the input data (X) to be provided with a specific array structure in the form of: [samples, time steps, features].\n",
    "\n",
    "Currently, our data is in the form: [samples, features] and we are framing the problem as one time step for each sample. We can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = trainX[:,:,np.newaxis]\n",
    "testX = testX[:,:,np.newaxis]\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May be useful to test the code on shorter data set...\n",
    "\n",
    "# trainX=trainX[:200,:,:]\n",
    "# trainY=trainY[:200]\n",
    "# testX=testX[:200,:,:]\n",
    "# testY=testY[:200]\n",
    "# trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original datasets\n",
    "trainX_orig, trainY_orig, testX_orig, testY_orig,= trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn a LSTM model \n",
    "\n",
    "A batch method is used during the optization phase. The size of the batches is set here to 256 samples (256 consecutive -in time- vectors X of look_back (=20) numerical values, and correponding outputs Y.\n",
    "\n",
    "The cell states are initialized at random. Obviously, the paramater shuffle has to be set to shuffle=False, as shuffling the data would destroy all time dependencies that are seeked by the LSTM. \n",
    "\n",
    "As by default, cell states are not reinitialized after an epoch, we prefer here to force these states to reinitilaize afetr each epoch.\n",
    "(Cell states are reinitialized, but not the internal networks -remind that there are 4 of them in a LSTM- are kept to  their current value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import math\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "size_of_batch = 256\n",
    "\n",
    "# adapt length of trainX and trainY to be divisible by size_of_batch\n",
    "L = int(len(trainX) / 256) * 256\n",
    "trainX = trainX_orig[:L, :, :]\n",
    "trainY = trainY_orig[:L]\n",
    "\n",
    "trainDate = Temp.index[:L]\n",
    "df_train_scaled = pd.DataFrame(index=trainDate)\n",
    "df_train_scaled[\"Actual\"] = trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 10\n",
    "neurons = 8  # size of both hidden stat (h) and cell state (C)\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(\n",
    "        neurons,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        batch_input_shape=(size_of_batch, trainX.shape[1], 1),\n",
    "        stateful=True,\n",
    "    )\n",
    ")\n",
    "model.add(Dense(1))\n",
    "# use stochastic gradient optimizer\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usual to reset the hidden states of RNN between epochs (technically, we should even do it between batches). See https://adgefficiency.com/tf2-lstm-hidden/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 5\n",
    "# Train the network\n",
    "for i in range(nb_epochs):\n",
    "    model.fit(\n",
    "        trainX, trainY, epochs=1, batch_size=size_of_batch, verbose=1, shuffle=False\n",
    "    )\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX, batch_size=size_of_batch)\n",
    "df_train_scaled[\"Predict\"] = trainPredict\n",
    "\n",
    "# adapt length of trainX and trainY to size_of_batch\n",
    "L_test = int(len(testX) / 256) * 256\n",
    "testX = testX_orig[:L_test, :, :]\n",
    "testY = testY_orig[:L_test]\n",
    "\n",
    "testDate = Temp.index[train_size : train_size + L_test]\n",
    "testPredict = model.predict(testX, batch_size=size_of_batch)\n",
    "df_test_scaled = pd.DataFrame(index=testDate)\n",
    "df_test_scaled[\"Actual\"] = testY\n",
    "df_test_scaled[\"Predict\"] = testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_test_scaled.plot(subplots=False, figsize=(18, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back in the original space\n",
    "To evaluate RMSE performance and visualize the results in the original space, one needs to rescale back the data to their original scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(df_train_scaled[\"Predict\"].values.reshape(-1, 1))\n",
    "trainY = scaler.inverse_transform(df_train_scaled[\"Actual\"].values.reshape(-1, 1))\n",
    "testPredict = scaler.inverse_transform(df_test_scaled[\"Predict\"].values.reshape(-1, 1))\n",
    "testY = scaler.inverse_transform(df_test_scaled[\"Actual\"].values.reshape(-1, 1))\n",
    "\n",
    "df_test = pd.DataFrame(index=testDate)\n",
    "df_test[\"Actual\"] = testY\n",
    "df_test[\"Predict\"] = testPredict\n",
    "df_test.plot(subplots=False, figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = np.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "r2_trainScore = r2_score(trainY, trainPredict)\n",
    "print(\n",
    "    \"Train Score:\\tRMSE is {:.2f} (°C),\\t R^2 is {:2.1f}%\".format(\n",
    "        trainScore, 100 * r2_trainScore\n",
    "    )\n",
    ")\n",
    "testScore = np.sqrt(mean_squared_error(testY, testPredict))\n",
    "r2_testScore = r2_score(testY, testPredict)\n",
    "print(\n",
    "    \"Test Score:\\tRMSE is {:.2f} (°C),\\t R^2 is {:2.1f}%\".format(testScore, 100 * r2_testScore)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
