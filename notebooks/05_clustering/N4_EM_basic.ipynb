{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nkeriven/ensta-mt12/blob/main/notebooks/06_clustering/N4_EM_basic.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM basic example \n",
    "The purpose of this labwork is to implement a Gaussian Mixture Model Clustering algorithm, using Expectation Maximization (EM) method. First, a code is proposed on a 1D example implementing directly the theoretical formula from the lecture. Second, the obtained results are compared with the results obtained using sklearn GMM function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import from matlab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/nkeriven/ensta-mt12/main/notebooks/data/fictitious_train.mat -O fictitious_train.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import scipy.stats as stats\n",
    "%matplotlib inline\n",
    "\n",
    "Data_train = loadmat('fictitious_train.mat')\n",
    "#Data_train.keys()\n",
    "\n",
    "X=Data_train.get('Xtrain')\n",
    "#H=Data_train.get('__header__')\n",
    "#print(\"dimensions ox X ={}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intialization of parameters and Kernel computation\n",
    "- note that here, the number of clusters is set a priori "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EM\n",
    "N=X.shape[0]\n",
    "K=2\n",
    "p=1\n",
    "MaxIter=100\n",
    "#init \n",
    "init=np.random.choice(range(N), size=K, replace=False)\n",
    "pivec= np.ones((K,p))/K \n",
    "muvec= np.zeros((K,p)) \n",
    "sigvec= np.zeros((K,p))\n",
    "postpr=np.zeros((N,K))\n",
    "\n",
    "for k in range(K):\n",
    "    muvec[k,:]=X[init[k],:]  #different means\n",
    "    sigvec[k,:]=np.var(X)/K #say, global variance divided by two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercize\n",
    "- Identify the arrays above wrt to the charateristics of the GMM introduced in the lecture\n",
    "- Explain why different means are initialized, whereas same variances may be used\n",
    "- Comment the line codes below, briefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A posteriori Proba to be in a class\n",
    "\n",
    "for t in range (0,MaxIter):\n",
    "    #E-Step\n",
    "    for i in range(0,N): \n",
    "        for k in range(K):\n",
    "            postpr[i,k]=pivec[k]* stats.norm.pdf( X[i], muvec[k,:],  np.sqrt( sigvec[k,:] ) )\n",
    "        postpr[i,:] /= postpr[i,:].sum()\n",
    "            \n",
    "    #M-step    \n",
    "    for k in range (0,K):\n",
    "        S = np.sum(postpr[:,k])\n",
    "        pivec[k,:]= S/N\n",
    "        muvec[k,:]= postpr[:,k].dot(X) / S\n",
    "        sigvec[k,:]=postpr[:,k].dot((X-muvec[k,:])**2) / S\n",
    "\n",
    "\n",
    "print(f'muvec={muvec}')\n",
    "print(f'sigvec={sigvec}')\n",
    "print(f'pivec={pivec}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "Below, the plot displays the 2 Gaussian pdfs involved in the mixture; the total pdf; the histogram of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xt=np.linspace(-2,8,1000)\n",
    "g = np.zeros_like(Xt)\n",
    "for k in range(K):\n",
    "    gmix = pivec[k]*stats.norm.pdf(Xt,muvec[k],np.sqrt(sigvec[k]))\n",
    "    g += gmix\n",
    "    plt.plot(Xt,gmix,label=f'g{k}')\n",
    "plt.plot(Xt, g, label='full density')\n",
    "plt.legend()\n",
    "plt.xlabel('X');\n",
    "plt.hist(X, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercize\n",
    "Increase K. Play with the initialization a bit. Comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "# Try GMMs using full covariance (no constraints imposed on cov)\n",
    "est = GaussianMixture(n_components=K, \n",
    "                      covariance_type='full', \n",
    "                      max_iter= MaxIter,\n",
    "                      random_state=0)\n",
    "\n",
    "est.fit(X)\n",
    "\n",
    "print(f'est.cov={est.covariances_}')\n",
    "print(f'est.means={est.means_}')\n",
    "print(f'est.weights={est.weights_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercize \n",
    "- compare the results obtained with Sklearn with the previously obtained results. \n",
    "- Comments? \n",
    "- Add the mixture pdf to the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt=np.linspace(-2,8,1000)\n",
    "gsk = np.zeros_like(Xt)\n",
    "for k in range(K):\n",
    "    gskmix=est.weights_[k]*stats.norm.pdf(Xt,np.squeeze(est.means_[k]),\n",
    "                  np.sqrt(np.squeeze(est.covariances_[k])))\n",
    "    gsk += gskmix\n",
    "    plt.plot(Xt,gskmix, label=f'gsk{k}')\n",
    "                         \n",
    "plt.plot(Xt, gsk, label='full density')\n",
    "plt.legend()\n",
    "plt.xlabel('X');\n",
    "plt.hist(X, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Kmeans vs EM\n",
    "g = np.zeros_like(Xt)\n",
    "gmix = dict()\n",
    "for k in range(K):\n",
    "    gmix[k] = pivec[k] * stats.norm.pdf(Xt,muvec[k],np.sqrt(sigvec[k]))\n",
    "    g += gmix[k]\n",
    "for k in range(K):\n",
    "    resp =  gmix[k] / g ;\n",
    "    plt.plot(Xt,resp,label=f'responsability class{k}')\n",
    "#plt.plot(Xt,resp1,label='responsability class1')\n",
    "\n",
    "est_km = KMeans(n_clusters = K, init = 'k-means++')\n",
    "Y=est_km.fit_predict(X)\n",
    "for k in range(K):\n",
    "    plt.scatter(X[Y==k],np.ones_like(X[Y==k])*.5,marker='+',\n",
    "                label=f'class{k}')\n",
    "#plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercize \n",
    "- Compare and interpret Kmeans wrt EM; \n",
    "- Interpret the responsability functions resp0 and resp1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
